import argparse
import hashlib
import logging
import os
import random
import re
import signal
import sys
import time
from datetime import datetime
from typing import List, Optional, Tuple, Dict
from urllib.parse import quote, urljoin, urlparse

import pymysql
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
try:
    from colorama import Fore, Back, Style, init
    # åˆå§‹åŒ–coloramaï¼Œæ”¯æŒWindowsç³»ç»Ÿ
    init(autoreset=True)
    COLORAMA_AVAILABLE = True
except ImportError:
    # å¦‚æœcoloramaä¸å¯ç”¨ï¼Œå®šä¹‰ç©ºçš„æ ·å¼
    class MockStyle:
        RESET_ALL = ''
    class MockFore:
        GREEN = RED = YELLOW = CYAN = MAGENTA = WHITE = ''
    class MockBack:
        GREEN = BLUE = ''
    
    Fore = MockFore()
    Back = MockBack()
    Style = MockStyle()
    COLORAMA_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡ï¼Œè®¾ç½®override=Falseï¼Œè¿™æ ·ç³»ç»Ÿç¯å¢ƒå˜é‡ä¼šä¼˜å…ˆäº.envæ–‡ä»¶ä¸­çš„å˜é‡
load_dotenv(override=False)

# åˆ¤æ–­å½“å‰ç¯å¢ƒ
ENVIRONMENT = os.getenv('ENVIRONMENT', 'development')
logger.info(f"å½“å‰è¿è¡Œç¯å¢ƒ: {ENVIRONMENT}")

# å¦‚æœæ˜¯ç”Ÿäº§ç¯å¢ƒï¼Œè¾“å‡ºä¸€ä¸ªæç¤º
if ENVIRONMENT == 'production':
    logger.info("æ­£åœ¨ä½¿ç”¨ç”Ÿäº§ç¯å¢ƒé…ç½®")
else:
    logger.info("æ­£åœ¨ä½¿ç”¨å¼€å‘ç¯å¢ƒé…ç½®")

# å½©è‰²æ—¥å¿—å·¥å…·ç±»
class ColorLogger:
    @staticmethod
    def success(message: str):
        """æˆåŠŸæ—¥å¿— - ç»¿è‰² + âœ…"""
        print(f"{Fore.GREEN}âœ… {message}{Style.RESET_ALL}")
        logger.info(message)
    
    @staticmethod
    def error(message: str):
        """é”™è¯¯æ—¥å¿— - çº¢è‰² + âŒ"""
        print(f"{Fore.RED}âŒ {message}{Style.RESET_ALL}")
        logger.error(message)
    
    @staticmethod
    def warning(message: str):
        """è­¦å‘Šæ—¥å¿— - é»„è‰² + âš ï¸"""
        print(f"{Fore.YELLOW}âš ï¸  {message}{Style.RESET_ALL}")
        logger.warning(message)
    
    @staticmethod
    def info(message: str):
        """ä¿¡æ¯æ—¥å¿— - é’è‰² + â„¹ï¸"""
        print(f"{Fore.CYAN}â„¹ï¸  {message}{Style.RESET_ALL}")
        logger.info(message)
    
    @staticmethod
    def processing(message: str):
        """å¤„ç†ä¸­æ—¥å¿— - ç´«è‰² + ğŸ”„"""
        print(f"{Fore.MAGENTA}ğŸ”„ {message}{Style.RESET_ALL}")
        logger.info(message)
    
    @staticmethod
    def found(message: str):
        """å‘ç°æ—¥å¿— - é»„è‰² + ğŸ”"""
        print(f"{Fore.YELLOW}ğŸ” {message}{Style.RESET_ALL}")
        logger.info(message)

# åˆ›å»ºå…¨å±€å½©è‰²æ—¥å¿—å®ä¾‹
color_log = ColorLogger()

# æ–­ç‚¹ç»­æŠ“ç®¡ç†ç±»
class CheckpointManager:
    def __init__(self, db_config: dict):
        self.db_config = db_config
        self.session_id = None
        self.checkpoint_data = None
        self.interrupted = False
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼Œä¼˜é›…å¤„ç†ä¸­æ–­
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        color_log.warning(f"æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
        self.interrupted = True
        if self.session_id:
            self.update_status('paused', 'ç”¨æˆ·ä¸­æ–­')
        sys.exit(0)
    
    def generate_session_id(self) -> str:
        """ç”Ÿæˆä¼šè¯ID"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        random_str = hashlib.md5(str(random.random()).encode()).hexdigest()[:8]
        return f"scraper_{timestamp}_{random_str}"
    
    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        try:
            return pymysql.connect(**self.db_config)
        except pymysql.Error as err:
            color_log.error(f"æ•°æ®åº“è¿æ¥é”™è¯¯: {err}")
            return None
    
    def create_checkpoint(self, total_records: int, resume_from: Optional[str] = None) -> str:
        """åˆ›å»ºæ–°çš„æ£€æŸ¥ç‚¹æˆ–æ¢å¤ç°æœ‰æ£€æŸ¥ç‚¹"""
        connection = self.get_db_connection()
        if not connection:
            return None
        
        try:
            cursor = connection.cursor()
            
            if resume_from:
                # æ¢å¤ç°æœ‰ä¼šè¯
                cursor.execute(
                    "SELECT * FROM scraper_checkpoint WHERE session_id = %s AND status IN ('running', 'paused')",
                    (resume_from,)
                )
                result = cursor.fetchone()
                if result:
                    self.session_id = resume_from
                    self.checkpoint_data = {
                        'id': result[0],
                        'session_id': result[1],
                        'last_processed_id': result[2],
                        'total_records': result[3],
                        'processed_records': result[4],
                        'success_count': result[5],
                        'failed_count': result[6],
                        'status': result[7]
                    }
                    # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
                    cursor.execute(
                        "UPDATE scraper_checkpoint SET status = 'running', last_update_time = NOW() WHERE session_id = %s",
                        (self.session_id,)
                    )
                    connection.commit()
                    color_log.success(f"æ¢å¤ä¼šè¯: {self.session_id}, ä»è®°å½•ID {self.checkpoint_data['last_processed_id']} ç»§ç»­")
                    return self.session_id
                else:
                    color_log.error(f"æœªæ‰¾åˆ°å¯æ¢å¤çš„ä¼šè¯: {resume_from}")
                    return None
            else:
                # åˆ›å»ºæ–°ä¼šè¯
                self.session_id = self.generate_session_id()
                cursor.execute(
                    """
                    INSERT INTO scraper_checkpoint 
                    (session_id, last_processed_id, total_records, processed_records, success_count, failed_count, status, notes)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    """,
                    (self.session_id, 0, total_records, 0, 0, 0, 'running', 'æ–°ä¼šè¯å¼€å§‹')
                )
                connection.commit()
                self.checkpoint_data = {
                    'last_processed_id': 0,
                    'total_records': total_records,
                    'processed_records': 0,
                    'success_count': 0,
                    'failed_count': 0,
                    'status': 'running'
                }
                color_log.success(f"åˆ›å»ºæ–°ä¼šè¯: {self.session_id}")
                return self.session_id
                
        except pymysql.Error as err:
            color_log.error(f"åˆ›å»ºæ£€æŸ¥ç‚¹é”™è¯¯: {err}")
            return None
        finally:
            if connection:
                cursor.close()
                connection.close()
    
    def update_progress(self, last_processed_id: int, success_count: int, failed_count: int):
        """æ›´æ–°è¿›åº¦"""
        if not self.session_id or self.interrupted:
            return
        
        connection = self.get_db_connection()
        if not connection:
            return
        
        try:
            cursor = connection.cursor()
            processed_records = success_count + failed_count
            cursor.execute(
                """
                UPDATE scraper_checkpoint 
                SET last_processed_id = %s, processed_records = %s, success_count = %s, failed_count = %s, last_update_time = NOW()
                WHERE session_id = %s
                """,
                (last_processed_id, processed_records, success_count, failed_count, self.session_id)
            )
            connection.commit()
            
            # æ›´æ–°æœ¬åœ°ç¼“å­˜
            self.checkpoint_data.update({
                'last_processed_id': last_processed_id,
                'processed_records': processed_records,
                'success_count': success_count,
                'failed_count': failed_count
            })
            
        except pymysql.Error as err:
            color_log.error(f"æ›´æ–°è¿›åº¦é”™è¯¯: {err}")
        finally:
            if connection:
                cursor.close()
                connection.close()
    
    def update_status(self, status: str, notes: str = ''):
        """æ›´æ–°çŠ¶æ€"""
        if not self.session_id:
            return
        
        connection = self.get_db_connection()
        if not connection:
            return
        
        try:
            cursor = connection.cursor()
            end_time = 'NOW()' if status in ['completed', 'failed'] else 'NULL'
            cursor.execute(
                f"""
                UPDATE scraper_checkpoint 
                SET status = %s, notes = %s, end_time = {end_time}, last_update_time = NOW()
                WHERE session_id = %s
                """,
                (status, notes, self.session_id)
            )
            connection.commit()
            
        except pymysql.Error as err:
            color_log.error(f"æ›´æ–°çŠ¶æ€é”™è¯¯: {err}")
        finally:
            if connection:
                cursor.close()
                connection.close()
    
    def get_resume_point(self) -> int:
        """è·å–æ¢å¤ç‚¹"""
        if self.checkpoint_data:
            return self.checkpoint_data['last_processed_id']
        return 0
    
    def get_progress_info(self) -> dict:
        """è·å–è¿›åº¦ä¿¡æ¯"""
        return self.checkpoint_data if self.checkpoint_data else {}
    
    def list_sessions(self):
        """åˆ—å‡ºæ‰€æœ‰ä¼šè¯"""
        connection = self.get_db_connection()
        if not connection:
            return
        
        try:
            cursor = connection.cursor()
            cursor.execute(
                """
                SELECT session_id, total_records, processed_records, success_count, failed_count, 
                       status, start_time, last_update_time, notes
                FROM scraper_checkpoint 
                ORDER BY start_time DESC
                LIMIT 20
                """
            )
            results = cursor.fetchall()
            
            if results:
                print(f"\n{Fore.CYAN}ğŸ“‹ æœ€è¿‘çš„æŠ“å–ä¼šè¯:{Style.RESET_ALL}")
                print(f"{Fore.WHITE}{'ä¼šè¯ID':<30} {'çŠ¶æ€':<10} {'è¿›åº¦':<15} {'æˆåŠŸ/å¤±è´¥':<12} {'å¼€å§‹æ—¶é—´':<20}{Style.RESET_ALL}")
                print("-" * 90)
                
                for row in results:
                    session_id, total, processed, success, failed, status, start_time, update_time, notes = row
                    progress = f"{processed}/{total}" if total > 0 else "0/0"
                    success_fail = f"{success}/{failed}"
                    
                    status_color = {
                        'running': Fore.GREEN,
                        'paused': Fore.YELLOW,
                        'completed': Fore.CYAN,
                        'failed': Fore.RED
                    }.get(status, Fore.WHITE)
                    
                    print(f"{session_id:<30} {status_color}{status:<10}{Style.RESET_ALL} {progress:<15} {success_fail:<12} {start_time}")
            else:
                color_log.info("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æŠ“å–ä¼šè¯")
                
        except pymysql.Error as err:
            color_log.error(f"æŸ¥è¯¢ä¼šè¯é”™è¯¯: {err}")
        finally:
            if connection:
                cursor.close()
                connection.close()

class SupportOrganizationScraper:
    def __init__(self, resume_session: Optional[str] = None):
        # æ•°æ®åº“è¿æ¥é…ç½® - ä»ç¯å¢ƒå˜é‡è·å–
        # éªŒè¯å¿…éœ€çš„ç¯å¢ƒå˜é‡
        db_password = os.getenv('DB_APP_PASSWORD')
        if not db_password:
            color_log.error("DB_APP_PASSWORD environment variable is required")
            raise ValueError("DB_APP_PASSWORD environment variable is required")
            
        self.db_config = {
            "host": os.getenv('DB_HOST', 'localhost'),
            "user": os.getenv('DB_APP_USER', 'edm_app_user'),
            "password": db_password,
            "database": os.getenv('DB_NAME', 'edm'),
            "charset": "utf8mb4",
        }

        # ç½‘ç«™åŸºç¡€URL
        self.base_url = "https://www.gai-rou.com"
        self.search_url = "https://www.gai-rou.com/?s={}"

        # è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ja-JP,ja;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

        # åˆ›å»ºä¼šè¯
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        self.checkpoint_manager = CheckpointManager(self.db_config)
        self.resume_session = resume_session

    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        try:
            connection = pymysql.connect(**self.db_config)
            color_log.success(f"æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“ (ç¯å¢ƒ: {ENVIRONMENT})")
            return connection
        except pymysql.Error as err:
            color_log.error(f"æ•°æ®åº“è¿æ¥é”™è¯¯: {err}")
            return None

    def fetch_organizations(self, resume_from_id: int = 0) -> List[Tuple]:
        """ä»æ•°æ®åº“è·å–å…¨æ—¥æœ¬çš„æœºæ„ä¿¡æ¯"""
        connection = self.get_db_connection()
        if not connection:
            return []

        try:
            cursor = connection.cursor()
            # å¦‚æœæœ‰æ¢å¤ç‚¹ï¼Œä»è¯¥ç‚¹ç»§ç»­
            where_clause = "WHERE (email IS NULL OR email = '') AND id > %s" if resume_from_id > 0 else "WHERE email IS NULL OR email = ''"
            query = f"""
            SELECT id, registration_number, organization_name, email, prefecture
            FROM support_organization_registry 
            {where_clause}
            ORDER BY id
            """
            
            if resume_from_id > 0:
                cursor.execute(query, (resume_from_id,))
                color_log.info(f"ä»è®°å½•ID {resume_from_id} æ¢å¤æŠ“å–")
            else:
                cursor.execute(query)
            
            results = cursor.fetchall()
            color_log.info(f"ä»æ•°æ®åº“è·å–åˆ° {len(results)} æ¡éœ€è¦æŠ“å–é‚®ä»¶çš„è®°å½•")
            return results
        except pymysql.Error as err:
            color_log.error(f"æŸ¥è¯¢æ•°æ®åº“é”™è¯¯: {err}")
            return []
        finally:
            if connection:
                cursor.close()
                connection.close()
    
    def get_total_records_count(self) -> int:
        """è·å–æ€»è®°å½•æ•°"""
        connection = self.get_db_connection()
        if not connection:
            return 0
        
        try:
            cursor = connection.cursor()
            cursor.execute("SELECT COUNT(*) FROM support_organization_registry WHERE email IS NULL OR email = ''")
            count = cursor.fetchone()[0]
            return count
        except pymysql.Error as err:
            color_log.error(f"æŸ¥è¯¢è®°å½•æ€»æ•°é”™è¯¯: {err}")
            return 0
        finally:
            if connection:
                cursor.close()
                connection.close()

    def search_organization(self, registration_number: str) -> Optional[str]:
        """åœ¨ç½‘ç«™ä¸Šæœç´¢æœºæ„å¹¶è·å–è¯¦æƒ…é¡µé¢URL"""
        try:
            # æ„å»ºæœç´¢URL
            search_url = self.search_url.format(quote(registration_number))
            color_log.processing(f"æœç´¢URL: {search_url}")

            # è¯·æ±‚æœç´¢é¡µé¢
            response = self.session.get(search_url, timeout=10)
            response.raise_for_status()

            # è§£æHTML
            soup = BeautifulSoup(response.content, "html.parser")

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾è¯¦æƒ…é¡µé¢é“¾æ¥
            # å¯»æ‰¾ç±»ä¼¼ https://www.gai-rou.com/shien/æ•°å­—/ çš„é“¾æ¥
            detail_pattern = re.compile(r"https://www\.gai-rou\.com/shien/(\d+)/?")

            # åœ¨æ‰€æœ‰é“¾æ¥ä¸­æœç´¢
            for link in soup.find_all("a", href=True):
                href = link["href"]
                if detail_pattern.match(href):
                    color_log.found(f"æ‰¾åˆ°è¯¦æƒ…é¡µé¢é“¾æ¥: {href}")
                    return href

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œæ•´URLï¼Œå°è¯•æŸ¥æ‰¾ç›¸å¯¹è·¯å¾„
            relative_pattern = re.compile(r"/shien/(\d+)/?")
            for link in soup.find_all("a", href=True):
                href = link["href"]
                match = relative_pattern.match(href)
                if match:
                    full_url = urljoin(self.base_url, href)
                    color_log.found(f"æ‰¾åˆ°è¯¦æƒ…é¡µé¢é“¾æ¥ (ç›¸å¯¹è·¯å¾„): {full_url}")
                    return full_url

            color_log.warning(f"æœªæ‰¾åˆ° {registration_number} çš„è¯¦æƒ…é¡µé¢é“¾æ¥")
            return None

        except requests.RequestException as e:
            color_log.error(f"è¯·æ±‚æœç´¢é¡µé¢å¤±è´¥ {registration_number}: {e}")
            return None
        except Exception as e:
            color_log.error(f"æœç´¢è¿‡ç¨‹å‡ºé”™ {registration_number}: {e}")
            return None

    def extract_email_and_website(self, detail_url: str) -> Tuple[Optional[str], Optional[str]]:
        """ä»è¯¦æƒ…é¡µé¢æå–emailåœ°å€å’Œç½‘ç«™URL"""
        try:
            color_log.processing(f"è·å–è¯¦æƒ…é¡µé¢: {detail_url}")
            response = self.session.get(detail_url, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")
            email = None
            website_url = None

            # 1. é¦–å…ˆå°è¯•æå–emailåœ°å€
            email = self.extract_email_from_page(soup)
            
            # 2. æå–ç½‘ç«™URL
            website_url = self.extract_website_from_page(soup)
            
            return email, website_url

        except requests.RequestException as e:
            color_log.error(f"è¯·æ±‚è¯¦æƒ…é¡µé¢å¤±è´¥ {detail_url}: {e}")
            return None, None
        except Exception as e:
            color_log.error(f"æå–ä¿¡æ¯è¿‡ç¨‹å‡ºé”™ {detail_url}: {e}")
            return None, None
    
    def extract_email_from_page(self, soup: BeautifulSoup) -> Optional[str]:
        """ä»é¡µé¢ä¸­æå–emailåœ°å€çš„ä¼˜åŒ–ç®—æ³•"""
        # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«"ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹"ã€"E-mail"ã€"Email"ç­‰å…³é”®è¯çš„å…ƒç´ 
        email_keywords = ["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", "E-mail", "Email", "email", "é€£çµ¡å…ˆ"]
        
        for keyword in email_keywords:
            email_elements = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
            for element in email_elements:
                parent = element.parent
                if parent:
                    # åœ¨çˆ¶å…ƒç´ åŠå…¶å…„å¼Ÿå…ƒç´ ä¸­æŸ¥æ‰¾email
                    for sibling in parent.find_next_siblings(limit=3):
                        email = self.find_email_in_element(sibling)
                        if email:
                            color_log.found(f"é€šè¿‡å…³é”®è¯'{keyword}'æ‰¾åˆ°email: {email}")
                            return email
                    
                    # åœ¨çˆ¶å…ƒç´ çš„ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ ä¸­æŸ¥æ‰¾
                    next_sibling = parent.find_next_sibling()
                    if next_sibling:
                        email = self.find_email_in_element(next_sibling)
                        if email:
                            color_log.found(f"é€šè¿‡å…³é”®è¯'{keyword}'åœ¨å…„å¼Ÿå…ƒç´ ä¸­æ‰¾åˆ°email: {email}")
                            return email
        
        # æ–¹æ³•2: æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥ä¸­çš„mailtoé“¾æ¥
        mailto_links = soup.find_all('a', href=re.compile(r'^mailto:', re.IGNORECASE))
        for link in mailto_links:
            href = link.get('href', '')
            if href.startswith('mailto:'):
                email = href.replace('mailto:', '').split('?')[0]  # ç§»é™¤æŸ¥è¯¢å‚æ•°
                if self.validate_email(email):
                    color_log.found(f"ä»mailtoé“¾æ¥æ‰¾åˆ°email: {email}")
                    return email
        
        # æ–¹æ³•3: åœ¨æ•´ä¸ªé¡µé¢æ–‡æœ¬ä¸­æœç´¢emailæ¨¡å¼ï¼ˆä½†æ’é™¤å¸¸è§çš„æ— ç”¨emailï¼‰
        page_text = soup.get_text()
        email = self.find_email_in_text(page_text)
        if email and not self.is_generic_email(email):
            color_log.found(f"åœ¨é¡µé¢æ–‡æœ¬ä¸­æ‰¾åˆ°email: {email}")
            return email
        
        return None
    
    def extract_website_from_page(self, soup: BeautifulSoup) -> Optional[str]:
        """ä»é¡µé¢ä¸­æå–ç½‘ç«™URL"""
        # æŸ¥æ‰¾åŒ…å«"ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"ã€"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ"ã€"URL"ç­‰å…³é”®è¯çš„å…ƒç´ 
        website_keywords = ["ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸", "ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ", "URL", "Website", "ã‚µã‚¤ãƒˆ"]
        
        for keyword in website_keywords:
            website_elements = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
            for element in website_elements:
                parent = element.parent
                if parent:
                    # åœ¨çˆ¶å…ƒç´ åŠå…¶å…„å¼Ÿå…ƒç´ ä¸­æŸ¥æ‰¾URL
                    for sibling in parent.find_next_siblings(limit=3):
                        url = self.find_url_in_element(sibling)
                        if url:
                            color_log.found(f"é€šè¿‡å…³é”®è¯'{keyword}'æ‰¾åˆ°ç½‘ç«™: {url}")
                            return url
        
        # æŸ¥æ‰¾æ‰€æœ‰å¤–éƒ¨é“¾æ¥
        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href', '')
            if self.is_external_website(href):
                color_log.found(f"æ‰¾åˆ°å¤–éƒ¨ç½‘ç«™é“¾æ¥: {href}")
                return href
        
        return None
    
    def find_url_in_element(self, element) -> Optional[str]:
        """åœ¨HTMLå…ƒç´ ä¸­æŸ¥æ‰¾URL"""
        if element:
            # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯é“¾æ¥å…ƒç´ 
            if element.name == 'a' and element.get('href'):
                href = element.get('href')
                if self.is_external_website(href):
                    return href
            
            # åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾URL
            text = element.get_text()
            url = self.find_url_in_text(text)
            if url:
                return url
        return None
    
    def find_url_in_text(self, text: str) -> Optional[str]:
        """åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾URL"""
        url_pattern = re.compile(
            r'https?://[\w\-\.]+(:[0-9]+)?(/[\w\-\._~:/?#[\]@!$&\'()*+,;=]*)?',
            re.IGNORECASE
        )
        matches = url_pattern.findall(text)
        
        for match in matches:
            if isinstance(match, tuple):
                url = match[0] + (match[1] if match[1] else '') + (match[2] if match[2] else '')
            else:
                url = match
            
            if self.is_external_website(url):
                return url
        return None
    
    def is_external_website(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å¤–éƒ¨ç½‘ç«™URL"""
        if not url or not url.startswith(('http://', 'https://')):
            return False
        
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # æ’é™¤gai-rou.comåŸŸå
            if 'gai-rou.com' in domain:
                return False
            
            # æ’é™¤å¸¸è§çš„ç¤¾äº¤åª’ä½“å’Œæœç´¢å¼•æ“
            excluded_domains = [
                'facebook.com', 'twitter.com', 'instagram.com', 'youtube.com',
                'google.com', 'yahoo.co.jp', 'bing.com'
            ]
            
            for excluded in excluded_domains:
                if excluded in domain:
                    return False
            
            return True
        except:
            return False
    
    def is_generic_email(self, email: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯é€šç”¨é‚®ç®±åœ°å€ï¼ˆéœ€è¦æ’é™¤çš„ï¼‰"""
        generic_patterns = [
            r'.*@(example|test|sample)\.com',
            r'.*@gmail\.com',  # å¯é€‰æ‹©æ˜¯å¦æ’é™¤gmail
            r'.*@yahoo\.(com|co\.jp)',
            r'.*@hotmail\.com',
            r'noreply@.*',
            r'no-reply@.*'
        ]
        
        for pattern in generic_patterns:
            if re.match(pattern, email.lower()):
                return True
        return False
    
    def scrape_email_from_website(self, website_url: str, max_pages: int = 3) -> Optional[str]:
        """ä»å…¬å¸å®˜ç½‘æŠ“å–é‚®ä»¶åœ°å€"""
        if not website_url:
            return None
        
        try:
            color_log.processing(f"å¼€å§‹ä»å®˜ç½‘æŠ“å–é‚®ä»¶: {website_url}")
            
            # è¦æ£€æŸ¥çš„é¡µé¢åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            pages_to_check = []
            
            # 1. ä¸»é¡µ
            pages_to_check.append(website_url.rstrip('/'))
            
            # 2. å¸¸è§çš„è”ç³»é¡µé¢
            contact_paths = [
                '/contact', '/contact.html', '/contact.php', '/contact.htm',
                '/ãŠå•ã„åˆã‚ã›', '/ã‚³ãƒ³ã‚¿ã‚¯ãƒˆ',
                '/about', '/about.html', '/about.php', '/about.htm',
                '/company', '/company.html', '/company.php',
                '/access', '/access.html'
            ]
            
            base_url = website_url.rstrip('/')
            for path in contact_paths[:max_pages-1]:  # é™¤äº†ä¸»é¡µå¤–çš„å…¶ä»–é¡µé¢
                pages_to_check.append(base_url + path)
            
            # æ£€æŸ¥æ¯ä¸ªé¡µé¢
            for page_url in pages_to_check:
                email = self.extract_email_from_website_page(page_url)
                if email:
                    color_log.success(f"åœ¨å®˜ç½‘é¡µé¢æ‰¾åˆ°é‚®ä»¶: {email} (é¡µé¢: {page_url})")
                    return email
                
                # æ¯ä¸ªé¡µé¢ä¹‹é—´æ·»åŠ å°çš„å»¶è¿Ÿ
                time.sleep(random.uniform(0.2, 0.5))
            
            color_log.warning(f"æœªåœ¨å®˜ç½‘æ‰¾åˆ°é‚®ä»¶åœ°å€: {website_url}")
            return None
            
        except Exception as e:
            color_log.error(f"ä»å®˜ç½‘æŠ“å–é‚®ä»¶æ—¶å‡ºé”™ {website_url}: {e}")
            return None
    
    def extract_email_from_website_page(self, page_url: str) -> Optional[str]:
        """ä»å•ä¸ªç½‘ç«™é¡µé¢æå–é‚®ä»¶åœ°å€"""
        try:
            logger.debug(f"æ£€æŸ¥é¡µé¢: {page_url}")
            response = self.session.get(page_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, "html.parser")
            
            # ä½¿ç”¨ç›¸åŒçš„é‚®ä»¶æå–ç®—æ³•
            email = self.extract_email_from_page(soup)
            
            if email and not self.is_generic_email(email):
                return email
            
            return None
            
        except requests.RequestException as e:
            logger.debug(f"è¯·æ±‚é¡µé¢å¤±è´¥ {page_url}: {e}")
            return None
        except Exception as e:
            logger.debug(f"å¤„ç†é¡µé¢æ—¶å‡ºé”™ {page_url}: {e}")
            return None
    
    def retry_request(self, func, *args, max_retries: int = 3, **kwargs):
        """é‡è¯•æœºåˆ¶è£…é¥°å™¨"""
        for attempt in range(max_retries):
            try:
                return func(*args, **kwargs)
            except requests.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = random.uniform(1, 3) * (attempt + 1)
                    color_log.warning(f"è¯·æ±‚å¤±è´¥ï¼Œ{wait_time:.1f}ç§’åé‡è¯• (ç¬¬{attempt + 1}æ¬¡): {e}")
                    time.sleep(wait_time)
                else:
                    color_log.error(f"é‡è¯•{max_retries}æ¬¡åä»ç„¶å¤±è´¥: {e}")
                    raise e
            except Exception as e:
                color_log.error(f"éç½‘ç»œé”™è¯¯ï¼Œä¸é‡è¯•: {e}")
                raise e
        return None
    
    def _estimate_remaining_time(self, completed: int, total: int, elapsed_time: float) -> str:
        """ä¼°ç®—å‰©ä½™æ—¶é—´"""
        if completed == 0:
            return "æœªçŸ¥"
        
        avg_time_per_item = elapsed_time / completed
        remaining_items = total - completed
        remaining_seconds = avg_time_per_item * remaining_items
        
        if remaining_seconds < 60:
            return f"{remaining_seconds:.0f}ç§’"
        elif remaining_seconds < 3600:
            return f"{remaining_seconds/60:.1f}åˆ†é’Ÿ"
        else:
            hours = remaining_seconds // 3600
            minutes = (remaining_seconds % 3600) // 60
            return f"{hours:.0f}å°æ—¶{minutes:.0f}åˆ†é’Ÿ"

    def find_email_in_element(self, element) -> Optional[str]:
        """åœ¨HTMLå…ƒç´ ä¸­æŸ¥æ‰¾emailåœ°å€"""
        if element:
            text = element.get_text()
            return self.find_email_in_text(text)
        return None

    def find_email_in_text(self, text: str) -> Optional[str]:
        """åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾emailåœ°å€"""
        # emailæ­£åˆ™è¡¨è¾¾å¼
        email_pattern = re.compile(
            r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"
        )
        matches = email_pattern.findall(text)

        if matches:
            # è¿”å›ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„emailåœ°å€
            email = matches[0]
            if self.validate_email(email):
                return email
        return None

    def validate_email(self, email: str) -> bool:
        """éªŒè¯emailåœ°å€çš„åˆæ³•æ€§"""
        pattern = re.compile(r"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}$")
        return bool(pattern.match(email))

    def update_email_in_db(self, org_id: int, email: str) -> bool:
        """æ›´æ–°æ•°æ®åº“ä¸­çš„emailåœ°å€"""
        connection = self.get_db_connection()
        if not connection:
            return False

        try:
            cursor = connection.cursor()
            query = "UPDATE support_organization_registry SET email = %s WHERE id = %s"
            cursor.execute(query, (email, org_id))
            connection.commit()

            if cursor.rowcount > 0:
                color_log.success(f"æˆåŠŸæ›´æ–°æœºæ„ID {org_id} çš„email: {email}")
                return True
            else:
                color_log.warning(f"æ›´æ–°æœºæ„ID {org_id} çš„emailå¤±è´¥ï¼šæœªæ‰¾åˆ°è®°å½•")
                return False

        except pymysql.Error as err:
            color_log.error(f"æ›´æ–°æ•°æ®åº“é”™è¯¯: {err}")
            connection.rollback()
            return False
        finally:
            if connection:
                cursor.close()
                connection.close()

    def process_organizations(self):
        """å¤„ç†æ‰€æœ‰æœºæ„çš„ä¸»æµç¨‹"""
        # åˆå§‹åŒ–æ£€æŸ¥ç‚¹
        total_records = self.get_total_records_count()
        if total_records == 0:
            color_log.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„è®°å½•")
            return
        
        # åˆ›å»ºæˆ–æ¢å¤æ£€æŸ¥ç‚¹
        session_id = self.checkpoint_manager.create_checkpoint(total_records, self.resume_session)
        if not session_id:
            color_log.error("æ— æ³•åˆ›å»ºæˆ–æ¢å¤æ£€æŸ¥ç‚¹")
            return
        
        # è·å–æ¢å¤ç‚¹å’Œè¿›åº¦ä¿¡æ¯
        resume_from_id = self.checkpoint_manager.get_resume_point()
        progress_info = self.checkpoint_manager.get_progress_info()
        
        # è·å–æœºæ„åˆ—è¡¨ï¼ˆä»æ¢å¤ç‚¹å¼€å§‹ï¼‰
        organizations = self.fetch_organizations(resume_from_id)
        if not organizations:
            if resume_from_id > 0:
                color_log.success("æ‰€æœ‰è®°å½•å·²å¤„ç†å®Œæˆ")
                self.checkpoint_manager.update_status('completed', 'æ‰€æœ‰è®°å½•å¤„ç†å®Œæˆ')
            else:
                color_log.error("æœªè·å–åˆ°ä»»ä½•æœºæ„æ•°æ®")
            return

        # åˆå§‹åŒ–è®¡æ•°å™¨ï¼ˆè€ƒè™‘æ¢å¤çš„æƒ…å†µï¼‰
        success_count = progress_info.get('success_count', 0)
        failed_count = progress_info.get('failed_count', 0)
        processed_count = 0
        current_batch_count = 0  # å½“å‰æ‰¹æ¬¡è®¡æ•°å™¨
        
        # è®¡ç®—æ€»ä½“è¿›åº¦ä¿¡æ¯
        already_processed = progress_info.get('processed_records', 0)
        current_batch_size = len(organizations)
        
        color_log.info(f"å¼€å§‹å¤„ç† {current_batch_size} æ¡è®°å½• (ä¼šè¯: {session_id})")
        color_log.info(f"ğŸ“Š æ€»ä½“è¿›åº¦: å·²å®Œæˆ {already_processed}/{total_records} ({(already_processed/total_records*100):.1f}%)")
        if resume_from_id > 0:
            color_log.info(f"ğŸ“ˆ ç´¯è®¡ç»Ÿè®¡: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")

        # è®°å½•æ‰¹æ¬¡å¼€å§‹æ—¶é—´
        self._batch_start_time = time.time()
        
        try:
            for (
                org_id,
                registration_number,
                organization_name,
                current_email,
                prefecture,
            ) in organizations:
                # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
                if self.checkpoint_manager.interrupted:
                    break
                
                processed_count += 1
                current_batch_count += 1
                
                # è®¡ç®—å½“å‰æ•´ä½“è¿›åº¦
                current_total_processed = already_processed + current_batch_count
                overall_progress = (current_total_processed / total_records) * 100
                batch_progress = (current_batch_count / current_batch_size) * 100
                
                color_log.processing(
                    f"ğŸ”„ [{current_batch_count}/{current_batch_size}] ({batch_progress:.1f}%) | æ€»è¿›åº¦: [{current_total_processed}/{total_records}] ({overall_progress:.1f}%) | {organization_name} ({prefecture})"
                )

                # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æœ‰æ•ˆçš„emailåœ°å€
                if current_email and current_email.strip():
                    color_log.info(
                        f"â­ï¸  æœºæ„ {organization_name} å·²æœ‰emailåœ°å€: {current_email}ï¼Œè·³è¿‡"
                    )
                    continue

                try:
                    # æœç´¢æœºæ„è¯¦æƒ…é¡µé¢
                    detail_url = self.retry_request(self.search_organization, registration_number)
                    if not detail_url:
                        color_log.warning(f"æœªæ‰¾åˆ°æœºæ„ {organization_name} çš„è¯¦æƒ…é¡µé¢")
                        failed_count += 1
                        continue

                    # ä» gai-rou.com æå–emailå’Œç½‘ç«™
                    email, website_url = self.retry_request(self.extract_email_and_website, detail_url)
                    
                    # å¦‚æœåœ¨ gai-rou.com æ²¡æ‰¾åˆ°emailï¼Œå°è¯•ä»å®˜ç½‘æŠ“å–
                    if not email and website_url:
                        color_log.info(f"åœ¨gai-rou.comæœªæ‰¾åˆ°emailï¼Œå°è¯•ä»å®˜ç½‘æŠ“å–: {website_url}")
                        email = self.scrape_email_from_website(website_url)
                    
                    if not email:
                        color_log.error(f"æœªæ‰¾åˆ°æœºæ„ {organization_name} çš„emailåœ°å€")
                        failed_count += 1
                        continue

                    # æ›´æ–°æ•°æ®åº“
                    if self.update_email_in_db(org_id, email):
                        success_count += 1
                        source = "gai-rou.com" if not website_url else "å®˜ç½‘"
                        color_log.success(f"âœ¨ æˆåŠŸå¤„ç†æœºæ„ {organization_name}: {email} (æ¥æº: {source})")
                    else:
                        failed_count += 1
                        color_log.error(f"æ›´æ–°æœºæ„ {organization_name} çš„emailå¤±è´¥")

                    # æ¯å¤„ç†5æ¡è®°å½•æ›´æ–°ä¸€æ¬¡è¿›åº¦
                    if current_batch_count % 5 == 0:
                        self.checkpoint_manager.update_progress(org_id, success_count, failed_count)
                        current_total_processed = already_processed + current_batch_count
                        overall_progress = (current_total_processed / total_records) * 100
                        success_rate = (success_count / (success_count + failed_count) * 100) if (success_count + failed_count) > 0 else 0
                        
                        print(f"\n{Fore.CYAN}ğŸ“Š è¿›åº¦æŠ¥å‘Š - æ‰¹æ¬¡: {current_batch_count}/{current_batch_size} | æ€»ä½“: {current_total_processed}/{total_records} ({overall_progress:.1f}%){Style.RESET_ALL}")
                        print(f"{Fore.GREEN}   âœ… æˆåŠŸ: {success_count} | {Fore.RED}âŒ å¤±è´¥: {failed_count} | {Fore.YELLOW}ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%{Style.RESET_ALL}")
                        print(f"{Fore.MAGENTA}   â±ï¸  é¢„è®¡å‰©ä½™: {self._estimate_remaining_time(current_batch_count, current_batch_size, time.time() - getattr(self, '_batch_start_time', time.time()))}{Style.RESET_ALL}\n")

                    # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    delay = random.uniform(0.5, 2.0)
                    time.sleep(delay)

                except Exception as e:
                    color_log.error(f"å¤„ç†æœºæ„ {organization_name} æ—¶å‡ºé”™: {e}")
                    failed_count += 1
                    continue

        except KeyboardInterrupt:
            color_log.warning("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
            self.checkpoint_manager.update_progress(org_id, success_count, failed_count)
            self.checkpoint_manager.update_status('paused', f'ç”¨æˆ·ä¸­æ–­ï¼Œå·²å¤„ç†{processed_count}æ¡è®°å½•')
            return
        except Exception as e:
            color_log.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            self.checkpoint_manager.update_status('failed', f'å¤„ç†é”™è¯¯: {str(e)}')
            return
        
        # æœ€ç»ˆæ›´æ–°è¿›åº¦
        if organizations:
            last_id = organizations[-1][0] if current_batch_count == len(organizations) else org_id
            self.checkpoint_manager.update_progress(last_id, success_count, failed_count)
            
            # æ˜¾ç¤ºæœ€ç»ˆæ‰¹æ¬¡ç»Ÿè®¡
            batch_time = time.time() - self._batch_start_time
            avg_time_per_record = batch_time / current_batch_count if current_batch_count > 0 else 0
            color_log.info(f"ğŸ“‹ æ‰¹æ¬¡å®Œæˆç»Ÿè®¡: å¤„ç† {current_batch_count} æ¡è®°å½•ï¼Œè€—æ—¶ {batch_time:.1f} ç§’ï¼Œå¹³å‡ {avg_time_per_record:.2f} ç§’/æ¡")

        # æœ€ç»ˆç»“æœç»Ÿè®¡
        total_this_session = current_batch_count
        total_overall = success_count + failed_count
        success_rate = (success_count / total_overall * 100) if total_overall > 0 else 0
        current_total_processed = already_processed + current_batch_count
        overall_completion = (current_total_processed / total_records) * 100
        
        # åˆ¤æ–­æ˜¯å¦å®Œæˆæ‰€æœ‰è®°å½•
        if len(organizations) < 100:  # å¦‚æœè¿”å›çš„è®°å½•å°‘äºé¢„æœŸï¼Œå¯èƒ½å·²ç»å®Œæˆ
            remaining_records = self.get_total_records_count()
            if remaining_records == 0:
                self.checkpoint_manager.update_status('completed', f'æ‰€æœ‰è®°å½•å¤„ç†å®Œæˆ')
                status_text = "ğŸ‰ å…¨éƒ¨å®Œæˆ"
                status_color = Fore.GREEN
            else:
                self.checkpoint_manager.update_status('paused', f'æœ¬æ¬¡å¤„ç†å®Œæˆï¼Œè¿˜æœ‰{remaining_records}æ¡è®°å½•å¾…å¤„ç†')
                status_text = "â¸ï¸ æ‰¹æ¬¡å®Œæˆ"
                status_color = Fore.YELLOW
        else:
            self.checkpoint_manager.update_status('paused', f'å¤„ç†äº†{total_this_session}æ¡è®°å½•')
            status_text = "â¸ï¸ æ‰¹æ¬¡å®Œæˆ" 
            status_color = Fore.YELLOW
        
        print(f"\n{Fore.WHITE}{Back.GREEN} {status_text} {Style.RESET_ALL}")
        print(f"{Fore.CYAN}ğŸ“… ä¼šè¯ID: {session_id}{Style.RESET_ALL}")
        print(f"{Fore.MAGENTA}ğŸ“Š æ€»ä½“è¿›åº¦: {current_total_processed}/{total_records} ({overall_completion:.1f}%){Style.RESET_ALL}")
        print(f"{Fore.GREEN}âœ… æœ¬æ¬¡æˆåŠŸ: {success_count - progress_info.get('success_count', 0)} | ç´¯è®¡æˆåŠŸ: {success_count}{Style.RESET_ALL}")
        print(f"{Fore.RED}âŒ æœ¬æ¬¡å¤±è´¥: {failed_count - progress_info.get('failed_count', 0)} | ç´¯è®¡å¤±è´¥: {failed_count}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}ğŸ“ˆ æ€»ä½“æˆåŠŸç‡: {success_rate:.1f}%{Style.RESET_ALL}")
        if overall_completion < 100:
            print(f"{Fore.YELLOW}ğŸ”„ è¦ç»§ç»­å¤„ç†ï¼Œè¯·ä½¿ç”¨: python scraper_with_checkpoint.py --resume {session_id}{Style.RESET_ALL}")
        
        color_log.success(f"å¤„ç†å®Œæˆï¼æœ¬æ¬¡: {total_this_session}æ¡, æ€»è¿›åº¦: {overall_completion:.1f}%, ç´¯è®¡æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}, æˆåŠŸç‡: {success_rate:.1f}%")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='EDMé‚®ä»¶åœ°å€æŠ“å–ç¨‹åº - æ”¯æŒæ–­ç‚¹ç»­æŠ“')
    parser.add_argument('--resume', type=str, help='æ¢å¤æŒ‡å®šä¼šè¯IDçš„æŠ“å–')
    parser.add_argument('--list-sessions', action='store_true', help='åˆ—å‡ºæ‰€æœ‰æŠ“å–ä¼šè¯')
    parser.add_argument('--no-color', action='store_true', help='ç¦ç”¨å½©è‰²è¾“å‡º')
    
    args = parser.parse_args()
    
    if args.no_color or not COLORAMA_AVAILABLE:
        # ç¦ç”¨å½©è‰²è¾“å‡º
        global Fore, Back, Style
        Fore = Back = Style = type('MockColor', (), {'__getattr__': lambda s, n: ''})()
    
    print(f"\n{Fore.CYAN}{'=' * 60}{Style.RESET_ALL}")
    print(f"{Fore.WHITE}{Back.BLUE}      ğŸš€ EDMé‚®ä»¶åœ°å€æŠ“å–ç¨‹åº (ç¯å¢ƒ: {ENVIRONMENT}) - æ”¯æŒæ–­ç‚¹ç»­æŠ“      {Style.RESET_ALL}")
    print(f"{Fore.CYAN}{'=' * 60}{Style.RESET_ALL}\n")
    
    scraper = SupportOrganizationScraper(resume_session=args.resume)
    
    # å¤„ç†åˆ—å‡ºä¼šè¯å‘½ä»¤
    if args.list_sessions:
        scraper.checkpoint_manager.list_sessions()
        return

    # æ˜¾ç¤ºå½“å‰æ•°æ®åº“é…ç½®
    print(f"{Fore.YELLOW}ğŸ“‹ å½“å‰æ•°æ®åº“é…ç½®:{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ä¸»æœº: {Fore.GREEN}{os.getenv('DB_HOST', 'localhost')}{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ æ•°æ®åº“: {Fore.GREEN}{os.getenv('DB_NAME', 'edm')}{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ç”¨æˆ·: {Fore.GREEN}{os.getenv('DB_APP_USER', 'edm_app_user')}{Style.RESET_ALL}")
    
    # ç¡®è®¤æ˜¯å¦ç»§ç»­
    print(f"\n{Fore.YELLOW}âš¡ åŠŸèƒ½ç‰¹æ€§:{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ğŸŒ æ”¯æŒå…¨æ—¥æœ¬åœ°åŒºæŠ“å–{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ğŸ”„ æ™ºèƒ½é‡è¯•æœºåˆ¶{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ğŸŒ å®˜ç½‘å¤‡ç”¨æŠ“å–{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ğŸ¯ éšæœºè¯·æ±‚é—´éš”{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ğŸ’¾ æ–­ç‚¹ç»­æŠ“æ”¯æŒ{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ğŸ“Š è¿›åº¦å®æ—¶ä¿å­˜{Style.RESET_ALL}")
    
    if args.resume:
        print(f"\n{Fore.GREEN}ğŸ”„ æ¢å¤ä¼šè¯: {args.resume}{Style.RESET_ALL}")
        response = "y"
    else:
        response = input(f"\n{Fore.CYAN}ğŸš€ æ˜¯å¦å¼€å§‹å¤„ç†? (y/N): {Style.RESET_ALL}").strip().lower()
    
    if response != "y":
        print(f"{Fore.YELLOW}â¹ï¸ æ“ä½œå–æ¶ˆ{Style.RESET_ALL}")
        return

    # å¼€å§‹å¤„ç†
    action_text = "æ¢å¤æŠ“å–" if args.resume else "å¼€å§‹æŠ“å–"
    print(f"\n{Fore.GREEN}ğŸ¯ {action_text}é‚®ä»¶åœ°å€...{Style.RESET_ALL}\n")
    
    # æ˜¾ç¤ºä½¿ç”¨æç¤º
    if not args.resume:
        print(f"{Fore.YELLOW}ğŸ’¡ æç¤º:{Style.RESET_ALL}")
        print(f"{Fore.WHITE}  â€¢ ä½¿ç”¨ Ctrl+C å¯ä»¥å®‰å…¨ä¸­æ–­å¹¶ä¿å­˜è¿›åº¦{Style.RESET_ALL}")
        print(f"{Fore.WHITE}  â€¢ ä½¿ç”¨ python scraper_with_checkpoint.py --list-sessions æŸ¥çœ‹æ‰€æœ‰ä¼šè¯{Style.RESET_ALL}")
        print(f"{Fore.WHITE}  â€¢ ä½¿ç”¨ python scraper_with_checkpoint.py --resume <ä¼šè¯ID> æ¢å¤æŠ“å–{Style.RESET_ALL}\n")
    
    scraper.process_organizations()


if __name__ == "__main__":
    main()