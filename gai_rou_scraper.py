#!/usr/bin/env python3
"""
GAI-ROU.COM æ”¯æ´æ©Ÿé–¢çˆ¬è™«è„šæœ¬
ä» https://www.gai-rou.com/shien_list/ çˆ¬å–ç™»å½•æ”¯æ´æœºå…³ä¿¡æ¯

ä½œè€…: Claude
æ—¥æœŸ: 2025-07-31
"""

import requests
from bs4 import BeautifulSoup
import pymysql
import time
import logging
from urllib.parse import urljoin, urlparse
import re
from typing import Dict, List, Optional
import os
from dotenv import load_dotenv

# é¢œè‰²æ”¯æŒ
try:
    from colorama import Fore, Back, Style, init
    # åˆå§‹åŒ–coloramaï¼Œæ”¯æŒWindowsç³»ç»Ÿ
    init(autoreset=True)
    COLORAMA_AVAILABLE = True
except ImportError:
    # å¦‚æœcoloramaä¸å¯ç”¨ï¼Œå®šä¹‰ç©ºçš„æ ·å¼
    class MockStyle:
        RESET_ALL = ''
    class MockFore:
        GREEN = RED = YELLOW = CYAN = MAGENTA = WHITE = ''
    class MockBack:
        GREEN = BLUE = ''
    
    Fore = MockFore()
    Back = MockBack()
    Style = MockStyle()
    COLORAMA_AVAILABLE = False

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å½©è‰²æ—¥å¿—ç±»
class ColorLogger:
    @staticmethod
    def success(message):
        print(f"{Fore.GREEN}âœ… {message}{Style.RESET_ALL}")
        logger.info(message)
    
    @staticmethod
    def error(message):
        print(f"{Fore.RED}âŒ {message}{Style.RESET_ALL}")
        logger.error(message)
    
    @staticmethod
    def warning(message):
        print(f"{Fore.YELLOW}âš ï¸  {message}{Style.RESET_ALL}")
        logger.warning(message)
    
    @staticmethod
    def info(message):
        print(f"{Fore.CYAN}â„¹ï¸  {message}{Style.RESET_ALL}")
        logger.info(message)
    
    @staticmethod
    def processing(message):
        print(f"{Fore.MAGENTA}ğŸ”„ {message}{Style.RESET_ALL}")
        logger.info(message)
    
    @staticmethod
    def found(message):
        print(f"{Fore.YELLOW}ğŸ” {message}{Style.RESET_ALL}")
        logger.info(message)

# åˆ›å»ºå…¨å±€å½©è‰²æ—¥å¿—å®ä¾‹
color_log = ColorLogger()

# é…ç½®åŸºç¡€æ—¥å¿—ï¼ˆåå¤‡ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/gai_rou_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class GaiRouScraper:
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.base_url = "https://www.gai-rou.com"
        self.list_url = "https://www.gai-rou.com/shien_list/"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # æ•°æ®åº“è¿æ¥å‚æ•°ï¼ˆä¸scraper_with_checkpoint.pyä¿æŒä¸€è‡´ï¼‰
        # è·å–æ•°æ®åº“å¯†ç 
        db_password = os.getenv('DB_APP_PASSWORD')
        if not db_password:
            color_log.error("ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡ DB_APP_PASSWORD")
            raise ValueError("DB_APP_PASSWORD environment variable is required")
            
        self.db_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'user': os.getenv('DB_APP_USER', 'edm_app_user'),
            'password': db_password,
            'database': os.getenv('DB_NAME', 'edm'),
            'charset': 'utf8mb4'
        }
        
        self.scraped_count = 0
        self.error_count = 0
        
        # æ˜¾ç¤ºæ•°æ®åº“è¿æ¥ä¿¡æ¯
        color_log.info(f"æ•°æ®åº“é…ç½®:")
        print(f"{Fore.WHITE}  ä¸»æœº: {Fore.GREEN}{self.db_config['host']}{Style.RESET_ALL}")
        print(f"{Fore.WHITE}  ç”¨æˆ·: {Fore.GREEN}{self.db_config['user']}{Style.RESET_ALL}")
        print(f"{Fore.WHITE}  æ•°æ®åº“: {Fore.GREEN}{self.db_config['database']}{Style.RESET_ALL}")

    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        try:
            return pymysql.connect(**self.db_config)
        except Exception as e:
            color_log.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise

    def get_page_content(self, url: str, max_retries: int = 3) -> Optional[BeautifulSoup]:
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return BeautifulSoup(response.text, 'html.parser')
            except Exception as e:
                color_log.warning(f"ç¬¬{attempt+1}æ¬¡å°è¯•è·å–é¡µé¢å¤±è´¥ {url}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    color_log.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
                    return None

    def extract_organization_list(self, soup: BeautifulSoup) -> List[Dict]:
        """ä»åˆ—è¡¨é¡µæå–æœºæ„ä¿¡æ¯"""
        organizations = []
        
        # GAI-ROU.COM ä½¿ç”¨ä¸åŒçš„ç»“æ„ï¼ŒæŸ¥æ‰¾æ‰€æœ‰åŒ…å« /shien/ é“¾æ¥çš„å…ƒç´ 
        org_links = soup.find_all('a', href=re.compile(r'/shien/\d+/'))
        
        for link in org_links:
            try:
                href = link.get('href')
                # æå–æœºæ„ID
                match = re.search(r'/shien/(\d+)/', href)
                if not match:
                    continue
                    
                org_id = match.group(1)
                full_url = urljoin(self.base_url, href)
                
                # æå–é“¾æ¥æ–‡æœ¬ä½œä¸ºæœºæ„åç§°
                org_name = link.get_text(strip=True)
                if not org_name:
                    org_name = f"Organization {org_id}"
                
                organizations.append({
                    'id': org_id,
                    'name': org_name,
                    'detail_url': full_url
                })
                
            except Exception as e:
                color_log.error(f"æå–æœºæ„ä¿¡æ¯å¤±è´¥: {e}")
                continue
                
        # å»é‡
        seen_ids = set()
        unique_orgs = []
        for org in organizations:
            if org['id'] not in seen_ids:
                seen_ids.add(org['id'])
                unique_orgs.append(org)
                
        return unique_orgs

    def extract_organization_detail(self, detail_url: str, org_id: str) -> Dict:
        """ä»è¯¦æƒ…é¡µæå–è¯¦ç»†ä¿¡æ¯"""
        soup = self.get_page_content(detail_url)
        if not soup:
            return {}
            
        detail_info = {}
        
        try:
            # GAI-ROU.COM ä½¿ç”¨æ–‡æœ¬æ ¼å¼è€Œéè¡¨æ ¼ï¼Œéœ€è¦è§£ææ–‡æœ¬å†…å®¹
            page_text = soup.get_text()
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„ä¸ªå­—æ®µ
            patterns = {
                'registration_number': r'ç™»éŒ²ç•ªå·[ï¼š:\s]*([^\s\n]+)',
                'registration_date': r'ç™»éŒ²å¹´æœˆæ—¥[ï¼š:\s]*([^\s\n]+)',
                'organization_name': r'æ©Ÿé–¢å[ï¼š:\s]*([^\n]+)',
                'postal_code': r'éƒµä¾¿ç•ªå·[ï¼š:\s]*([^\s\n]+)',
                'address': r'ä½æ‰€[ï¼š:\s]*([^\n]+)',
                'phone_number': r'é›»è©±ç•ªå·[ï¼š:\s]*([^\s\n]+)',
                'website': r'ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸[ï¼š:\s]*([^\s\n]+)',
                'support_languages': r'å¯¾å¿œè¨€èª[ï¼š:\s]*([^\n]+)',
                'support_content': r'æ”¯æ´æ¥­å‹™ã®å†…å®¹[ï¼š:\s]*([^\n]+)',
                'support_start_date': r'æ”¯æ´é–‹å§‹æ—¥[ï¼š:\s]*([^\s\n]+)'
            }
            
            for field, pattern in patterns.items():
                match = re.search(pattern, page_text)
                if match:
                    detail_info[field] = match.group(1).strip()
            
            # é¢å¤–çš„ç½‘ç«™URLæå–æ–¹æ³•
            website_url = self.extract_website_url(soup, page_text)
            if website_url:
                detail_info['website'] = website_url
            
            # ä»é¡µé¢æ ‡é¢˜æå–æœºæ„åç§°ï¼ˆå¤‡ç”¨ï¼‰
            title = soup.find('title')
            if title and not detail_info.get('organization_name'):
                title_text = title.get_text(strip=True)
                if '|' in title_text:
                    detail_info['organization_name'] = title_text.split('|')[0].strip()
            
            # åˆ¤æ–­æ”¯æ´ç±»å‹
            support_type = self.determine_support_type(page_text)
            detail_info['support_type'] = support_type
            
        except Exception as e:
            color_log.error(f"æå–è¯¦æƒ…ä¿¡æ¯å¤±è´¥ {detail_url}: {e}")
            
        return detail_info

    def extract_website_url(self, soup: BeautifulSoup, page_text: str) -> str:
        """ä»é¡µé¢ä¸­æå–ç½‘ç«™URL"""
        website_url = ""
        
        try:
            # æ–¹æ³•1: æŸ¥æ‰¾å¤–éƒ¨é“¾æ¥
            external_links = soup.find_all('a', href=True)
            for link in external_links:
                href = link.get('href')
                if href and any(domain in href for domain in ['http://', 'https://']):
                    # æ’é™¤gai-rou.comè‡ªèº«çš„é“¾æ¥
                    if 'gai-rou.com' not in href:
                        # æ¸…ç†URL
                        cleaned_url = self.clean_website_url(href)
                        if cleaned_url:
                            website_url = cleaned_url
                            break
            
            # æ–¹æ³•2: ä»æ–‡æœ¬ä¸­æå–URL
            if not website_url:
                url_patterns = [
                    r'https?://[^\s\n<>"\']+',
                    r'www\.[^\s\n<>"\']+',
                    r'[a-zA-Z0-9.-]+\.(com|co\.jp|jp|org|net|info)[^\s\n<>"\']*'
                ]
                
                for pattern in url_patterns:
                    matches = re.findall(pattern, page_text, re.IGNORECASE)
                    for match in matches:
                        # æ’é™¤gai-rou.comçš„é“¾æ¥
                        if 'gai-rou.com' not in match.lower():
                            cleaned_url = self.clean_website_url(match)
                            if cleaned_url:
                                website_url = cleaned_url
                                break
                    if website_url:
                        break
                        
        except Exception as e:
            color_log.error(f"æå–ç½‘ç«™URLå¤±è´¥: {e}")
            
        return website_url

    def clean_website_url(self, url: str) -> str:
        """æ¸…ç†å’Œæ ‡å‡†åŒ–ç½‘ç«™URL"""
        if not url:
            return ""
            
        try:
            # ç§»é™¤å‰åç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
            url = url.strip().rstrip('.,;:!')
            
            # å¦‚æœURLä¸ä»¥httpå¼€å¤´ï¼Œæ·»åŠ https
            if not url.startswith(('http://', 'https://')):
                if url.startswith('www.'):
                    url = 'https://' + url
                elif '.' in url and not url.startswith('//'):
                    url = 'https://' + url
                else:
                    return ""
            
            # éªŒè¯URLæ ¼å¼
            from urllib.parse import urlparse
            parsed = urlparse(url)
            if parsed.netloc and parsed.scheme in ['http', 'https']:
                return url
                
        except Exception as e:
            color_log.error(f"æ¸…ç†URLå¤±è´¥ '{url}': {e}")
            
        return ""

    def determine_support_type(self, page_text: str) -> str:
        """æ ¹æ®é¡µé¢å†…å®¹åˆ¤æ–­æ”¯æ´ç±»å‹"""
        text_lower = page_text.lower()
        
        has_tokutei = any(keyword in text_lower for keyword in [
            'ç‰¹å®šæŠ€èƒ½', 'tokutei', 'specified skilled worker'
        ])
        
        has_jisshu = any(keyword in text_lower for keyword in [
            'æŠ€èƒ½å®Ÿç¿’', 'æŠ€èƒ½å®ä¹ ', 'jisshu', 'technical intern'
        ])
        
        if has_tokutei and has_jisshu:
            return 'both'
        elif has_tokutei:
            return 'tokutei_ginou'
        elif has_jisshu:
            return 'ginou_jisshuusei'
        else:
            return 'both'  # é»˜è®¤ä¸¤è€…éƒ½æ”¯æŒ

    def extract_prefecture_from_address(self, address: str) -> str:
        """ä»åœ°å€ä¸­æå–éƒ½é“åºœå¿"""
        if not address:
            return ""
            
        prefectures = [
            'åŒ—æµ·é“', 'é’æ£®çœŒ', 'å²©æ‰‹çœŒ', 'å®®åŸçœŒ', 'ç§‹ç”°çœŒ', 'å±±å½¢çœŒ', 'ç¦å³¶çœŒ',
            'èŒ¨åŸçœŒ', 'æ ƒæœ¨çœŒ', 'ç¾¤é¦¬çœŒ', 'åŸ¼ç‰çœŒ', 'åƒè‘‰çœŒ', 'æ±äº¬éƒ½', 'ç¥å¥ˆå·çœŒ',
            'æ–°æ½ŸçœŒ', 'å¯Œå±±çœŒ', 'çŸ³å·çœŒ', 'ç¦äº•çœŒ', 'å±±æ¢¨çœŒ', 'é•·é‡çœŒ', 'å²é˜œçœŒ',
            'é™å²¡çœŒ', 'æ„›çŸ¥çœŒ', 'ä¸‰é‡çœŒ', 'æ»‹è³€çœŒ', 'äº¬éƒ½åºœ', 'å¤§é˜ªåºœ', 'å…µåº«çœŒ',
            'å¥ˆè‰¯çœŒ', 'å’Œæ­Œå±±çœŒ', 'é³¥å–çœŒ', 'å³¶æ ¹çœŒ', 'å²¡å±±çœŒ', 'åºƒå³¶çœŒ', 'å±±å£çœŒ',
            'å¾³å³¶çœŒ', 'é¦™å·çœŒ', 'æ„›åª›çœŒ', 'é«˜çŸ¥çœŒ', 'ç¦å²¡çœŒ', 'ä½è³€çœŒ', 'é•·å´çœŒ',
            'ç†Šæœ¬çœŒ', 'å¤§åˆ†çœŒ', 'å®®å´çœŒ', 'é¹¿å…å³¶çœŒ', 'æ²–ç¸„çœŒ'
        ]
        
        for prefecture in prefectures:
            if prefecture in address:
                return prefecture
                
        return ""

    def normalize_organization_name(self, name: str) -> str:
        """æ ‡å‡†åŒ–æœºæ„åç§°ç”¨äºé‡å¤æ£€æŸ¥"""
        if not name:
            return ""
            
        # æ¸…ç†ç©ºæ ¼
        normalized = name.strip()
        if not normalized:
            return ""
        
        # ç§»é™¤å¸¸è§çš„å…¬å¸å‰ç¼€å’Œåç¼€
        prefixes_to_remove = [
            'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'å”åŒçµ„åˆ', 'ä¸€èˆ¬ç¤¾å›£æ³•äºº', 'å…¬ç›Šç¤¾å›£æ³•äºº', 
            'ä¸€èˆ¬è²¡å›£æ³•äºº', 'å…¬ç›Šè²¡å›£æ³•äºº', 'åˆåŒä¼šç¤¾', 'åˆè³‡ä¼šç¤¾', 'åˆåä¼šç¤¾'
        ]
        
        suffixes_to_remove = [
            'æ ªå¼ä¼šç¤¾', '(æ ª)', 'æœ‰é™ä¼šç¤¾', '(æœ‰)', 'å”åŒçµ„åˆ', 'çµ„åˆ',
            'llc', 'inc', 'corp', 'co.,ltd', 'co.ltd', 'ltd'
        ]
        
        # ç§»é™¤å‰ç¼€
        for prefix in prefixes_to_remove:
            if normalized.startswith(prefix):
                normalized = normalized[len(prefix):].strip()
                break
        
        # ç§»é™¤åç¼€
        for suffix in suffixes_to_remove:
            if normalized.lower().endswith(suffix.lower()):
                normalized = normalized[:-len(suffix)].strip()
                break
        
        # è½¬æ¢ä¸ºå°å†™å¹¶ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ—¥æ–‡å­—ç¬¦ï¼‰
        import re
        normalized = normalized.lower()
        normalized = re.sub(r'[^\w\u3040-\u309f\u30a0-\u30ff\u4e00-\u9faf]', '', normalized)
        
        return normalized

    def check_organization_exists(self, org_data: Dict) -> bool:
        """æ£€æŸ¥æœºæ„æ˜¯å¦å·²å­˜åœ¨"""
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            org_name = org_data.get('organization_name', '').strip()
            if not org_name:
                return False
            
            # æ ‡å‡†åŒ–å½“å‰æœºæ„åç§°
            normalized_name = self.normalize_organization_name(org_name)
            if not normalized_name:
                return False
            
            # è·å–æ‰€æœ‰ç°æœ‰æœºæ„åç§°è¿›è¡Œæ¯”è¾ƒ
            cursor.execute("SELECT organization_name, registration_number FROM support_organization_registry")
            existing_orgs = cursor.fetchall()
            
            for existing_name, existing_reg_num in existing_orgs:
                if existing_name:
                    existing_normalized = self.normalize_organization_name(existing_name)
                    if existing_normalized == normalized_name:
                        color_log.info(f"å‘ç°é‡å¤æœºæ„åç§°: '{org_name}' ä¸ç°æœ‰ '{existing_name}' åŒ¹é…")
                        return True
            
            # å¦‚æœæœ‰ç™»éŒ²ç•ªå·ï¼Œä¹Ÿæ£€æŸ¥ç™»éŒ²ç•ªå·
            if org_data.get('registration_number'):
                cursor.execute(
                    "SELECT organization_name FROM support_organization_registry WHERE registration_number = %s",
                    (org_data['registration_number'],)
                )
                if cursor.fetchone():
                    color_log.info(f"ç™»éŒ²ç•ªå·å·²å­˜åœ¨: {org_data['registration_number']}")
                    return True
            
            return False
            
        except Exception as e:
            color_log.error(f"æ£€æŸ¥æœºæ„æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
            return False
        finally:
            if 'conn' in locals():
                conn.close()

    def save_organization(self, org_data: Dict) -> bool:
        """ä¿å­˜æœºæ„ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            # æ£€æŸ¥æœºæ„æ˜¯å¦å·²å­˜åœ¨
            if self.check_organization_exists(org_data):
                color_log.success(f"æœºæ„å·²å­˜åœ¨ï¼Œè·³è¿‡: {org_data.get('organization_name', 'Unknown')}")
                return True
            
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # æå–éƒ½é“åºœå¿
            prefecture = self.extract_prefecture_from_address(org_data.get('address', ''))
            
            # æ’å…¥æ•°æ®
            sql = """
                INSERT INTO support_organization_registry 
                (registration_number, registration_date, organization_name, address, prefecture, 
                 phone_number, representative_name, support_type, email, website, created_at, updated_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
            """
            
            values = (
                org_data.get('registration_number', ''),
                org_data.get('registration_date', None),
                org_data.get('organization_name', ''),
                org_data.get('address', ''),
                prefecture,
                org_data.get('phone_number', ''),
                org_data.get('representative_name', ''),
                org_data.get('support_type', 'both'),
                org_data.get('email', ''),
                org_data.get('website', '')
            )
            
            cursor.execute(sql, values)
            conn.commit()
            
            color_log.success(f"æˆåŠŸä¿å­˜æœºæ„: {org_data.get('organization_name', 'Unknown')}")
            return True
            
        except Exception as e:
            color_log.error(f"ä¿å­˜æœºæ„ä¿¡æ¯å¤±è´¥: {e}")
            return False
        finally:
            if 'conn' in locals():
                conn.close()

    def get_next_page_url(self, soup: BeautifulSoup) -> Optional[str]:
        """ä»å½“å‰é¡µé¢è·å–ä¸‹ä¸€é¡µçš„URL"""
        try:
            # æŸ¥æ‰¾ "next page" é“¾æ¥ - æ ¹æ®ä½ æä¾›çš„HTMLç»“æ„
            next_link = soup.find('a', class_='next page-numbers')
            if next_link and next_link.get('href'):
                next_url = next_link.get('href')
                # è½¬æ¢ä¸ºå®Œæ•´URL
                full_next_url = urljoin(self.base_url, next_url)
                color_log.found(f"å‘ç°ä¸‹ä¸€é¡µé“¾æ¥: {full_next_url}")
                return full_next_url
            
            # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾åŒ…å« ">" æˆ– "æ¬¡ã¸" çš„é“¾æ¥
            pagination_links = soup.find_all('a', href=re.compile(r'/page/\d+/'))
            for link in pagination_links:
                link_text = link.get_text(strip=True)
                if any(indicator in link_text for indicator in ['>', 'æ¬¡ã¸', 'Next', 'next']):
                    next_url = link.get('href')
                    full_next_url = urljoin(self.base_url, next_url)
                    color_log.found(f"é€šè¿‡æ–‡æœ¬åŒ¹é…å‘ç°ä¸‹ä¸€é¡µé“¾æ¥: {full_next_url}")
                    return full_next_url
            
            color_log.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µ")
            return None
            
        except Exception as e:
            color_log.error(f"æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥æ—¶å‡ºé”™: {e}")
            return None

    def get_total_pages(self) -> int:
        """è·å–æ€»é¡µæ•°ï¼ˆå·²åºŸå¼ƒï¼Œæ”¹ç”¨åŠ¨æ€åˆ†é¡µï¼‰"""
        color_log.warning("get_total_pagesæ–¹æ³•å·²åºŸå¼ƒï¼Œç°åœ¨ä½¿ç”¨åŠ¨æ€åˆ†é¡µ")
        return 1

    def scrape_all_organizations(self):
        """çˆ¬å–æ‰€æœ‰æœºæ„ä¿¡æ¯ - ä½¿ç”¨åŠ¨æ€åˆ†é¡µ"""
        print(f"\n{Fore.CYAN}{'=' * 60}{Style.RESET_ALL}")
        print(f"{Fore.WHITE}{Back.BLUE}      ğŸš€ GAI-ROU.COM æ”¯æ´æœºå…³çˆ¬è™«ç³»ç»Ÿ (åŠ¨æ€åˆ†é¡µ)      {Style.RESET_ALL}")
        print(f"{Fore.CYAN}{'=' * 60}{Style.RESET_ALL}\n")
        
        color_log.info("å¼€å§‹çˆ¬å– GAI-ROU.COM æ”¯æ´æœºå…³ä¿¡æ¯")
        color_log.info("ä½¿ç”¨åŠ¨æ€åˆ†é¡µæ¨¡å¼ï¼Œå°†è·Ÿéš'ä¸‹ä¸€é¡µ'é“¾æ¥ç›´åˆ°ç»“æŸ")
        
        # å¼€å§‹åŠ¨æ€åˆ†é¡µçˆ¬å–
        current_url = self.list_url
        page_number = 1
        max_pages = 1000  # è®¾ç½®æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œé˜²æ­¢æ— é™å¾ªç¯
        
        while current_url and page_number <= max_pages:
            try:
                color_log.processing(f"æ­£åœ¨å¤„ç†ç¬¬ {page_number} é¡µ: {current_url}")
                
                # è·å–å½“å‰é¡µé¢å†…å®¹
                soup = self.get_page_content(current_url)
                if not soup:
                    color_log.error(f"è·å–ç¬¬ {page_number} é¡µå¤±è´¥")
                    break
                
                # æå–æœºæ„åˆ—è¡¨
                organizations = self.extract_organization_list(soup)
                color_log.found(f"ç¬¬ {page_number} é¡µå‘ç° {len(organizations)} ä¸ªæœºæ„")
                
                # å¦‚æœå½“å‰é¡µæ²¡æœ‰æœºæ„ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ«å°¾
                if not organizations:
                    color_log.warning(f"ç¬¬ {page_number} é¡µæœªå‘ç°ä»»ä½•æœºæ„ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ«å°¾")
                    break
                
                # å¤„ç†æ¯ä¸ªæœºæ„
                page_success = 0
                page_failed = 0
                
                for i, org in enumerate(organizations, 1):
                    try:
                        color_log.processing(f"å¤„ç†ç¬¬ {page_number} é¡µç¬¬ {i}/{len(organizations)} ä¸ªæœºæ„: {org['name'][:50]}...")
                        
                        # è·å–è¯¦ç»†ä¿¡æ¯
                        detail_info = self.extract_organization_detail(org['detail_url'], org['id'])
                        
                        # åˆå¹¶ä¿¡æ¯
                        org_data = {
                            'organization_name': detail_info.get('organization_name', org['name']),
                            'address': detail_info.get('address', ''),
                            'registration_number': detail_info.get('registration_number', ''),
                            'registration_date': detail_info.get('registration_date', None),
                            'phone_number': detail_info.get('phone_number', ''),
                            'representative_name': detail_info.get('representative_name', ''),
                            'support_type': detail_info.get('support_type', 'both'),
                            'email': detail_info.get('email', ''),
                            'website': detail_info.get('website', ''),
                            'support_languages': detail_info.get('support_languages', ''),
                            'postal_code': detail_info.get('postal_code', '')
                        }
                        
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        if self.save_organization(org_data):
                            self.scraped_count += 1
                            page_success += 1
                        else:
                            self.error_count += 1
                            page_failed += 1
                            
                        # å»¶è¿Ÿé¿å…è¢«å°IP
                        time.sleep(1)
                        
                    except Exception as e:
                        color_log.error(f"å¤„ç†æœºæ„å¤±è´¥ {org['name']}: {e}")
                        self.error_count += 1
                        page_failed += 1
                        continue
                
                # é¡µé¢å¤„ç†å®Œæˆç»Ÿè®¡
                color_log.success(f"ç¬¬ {page_number} é¡µå¤„ç†å®Œæˆ - æˆåŠŸ: {page_success}, å¤±è´¥: {page_failed}")
                
                # æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥
                next_url = self.get_next_page_url(soup)
                if next_url:
                    current_url = next_url
                    page_number += 1
                    color_log.info(f"å‡†å¤‡å¤„ç†ä¸‹ä¸€é¡µ (ç¬¬ {page_number} é¡µ)")
                    time.sleep(3)  # é¡µé¢é—´å»¶è¿Ÿ
                else:
                    color_log.success("å·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œçˆ¬å–å®Œæˆ")
                    break
                
            except Exception as e:
                color_log.error(f"å¤„ç†ç¬¬ {page_number} é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                # å°è¯•æŸ¥æ‰¾ä¸‹ä¸€é¡µç»§ç»­
                if soup:
                    next_url = self.get_next_page_url(soup)
                    if next_url:
                        current_url = next_url
                        page_number += 1
                        continue
                break
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶
        if page_number > max_pages:
            color_log.warning(f"å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ ({max_pages} é¡µ)ï¼Œåœæ­¢çˆ¬å–")
        
        # æœ€ç»ˆç»Ÿè®¡
        print(f"\n{Fore.WHITE}{Back.GREEN} ğŸ‰ çˆ¬å–å®Œæˆ! {Style.RESET_ALL}")
        print(f"{Fore.CYAN}ğŸ“Š å¤„ç†é¡µæ•°: {page_number - 1} é¡µ{Style.RESET_ALL}")
        print(f"{Fore.GREEN}âœ… æˆåŠŸ: {self.scraped_count}{Style.RESET_ALL}")
        print(f"{Fore.RED}âŒ å¤±è´¥: {self.error_count}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}ğŸ“ˆ æ€»è®¡: {self.scraped_count + self.error_count}{Style.RESET_ALL}")
        
        if self.scraped_count + self.error_count > 0:
            success_rate = (self.scraped_count / (self.scraped_count + self.error_count)) * 100
            print(f"{Fore.MAGENTA}ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%{Style.RESET_ALL}")
        
        color_log.success(f"åŠ¨æ€åˆ†é¡µçˆ¬å–å®Œæˆ! å¤„ç† {page_number - 1} é¡µï¼ŒæˆåŠŸ: {self.scraped_count}, å¤±è´¥: {self.error_count}")

def main():
    """ä¸»å‡½æ•°"""
    scraper = GaiRouScraper()
    scraper.scrape_all_organizations()

if __name__ == "__main__":
    main()