#!/usr/bin/env python3
"""
GAI-ROU.COM çˆ¬è™«æ¼”ç¤ºè„šæœ¬
å®‰å…¨åœ°è¿è¡Œçˆ¬è™«ï¼Œå¤„ç†å°‘é‡é¡µé¢è¿›è¡Œæ¼”ç¤º

ä½œè€…: Claude
æ—¥æœŸ: 2025-07-31
"""

from gai_rou_scraper import GaiRouScraper
import logging

def run_demo_scrape(max_pages=2, max_orgs_per_page=2):
    """è¿è¡Œæ¼”ç¤ºçˆ¬å–ï¼Œé™åˆ¶é¡µé¢æ•°å’Œæ¯é¡µå¤„ç†çš„æœºæ„æ•°é‡"""
    
    print(f"=== GAI-ROU.COM çˆ¬è™«æ¼”ç¤º ===")
    print(f"å°†å¤„ç†æœ€å¤š {max_pages} é¡µï¼Œæ¯é¡µæœ€å¤š {max_orgs_per_page} ä¸ªæœºæ„")
    print()
    
    scraper = GaiRouScraper()
    saved_count = 0
    skipped_count = 0
    
    # ä½¿ç”¨åŠ¨æ€åˆ†é¡µæ¼”ç¤º
    current_url = scraper.list_url
    page_number = 1
    
    while current_url and page_number <= max_pages:
        try:
            page_url = current_url
                
            print(f"ğŸ“„ å¤„ç†ç¬¬ {page_number} é¡µ: {page_url}")
            
            soup = scraper.get_page_content(page_url)
            if not soup:
                print(f"âŒ è·å–ç¬¬ {page_number} é¡µå¤±è´¥")
                break
            
            # æå–æœºæ„åˆ—è¡¨
            organizations = scraper.extract_organization_list(soup)
            print(f"ğŸ” å‘ç° {len(organizations)} ä¸ªæœºæ„")
            
            # é™åˆ¶å¤„ç†æ•°é‡
            limited_orgs = organizations[:max_orgs_per_page]
            print(f"ğŸ“‹ å¤„ç†å‰ {len(limited_orgs)} ä¸ªæœºæ„")
            
            for i, org in enumerate(limited_orgs, 1):
                try:
                    print(f"  {i}. æ­£åœ¨å¤„ç†: {org['name']}")
                    
                    # è·å–è¯¦ç»†ä¿¡æ¯
                    detail_info = scraper.extract_organization_detail(org['detail_url'], org['id'])
                    
                    if detail_info:
                        print(f"     âœ… æå–æˆåŠŸ")
                        print(f"     ğŸ“‹ ç™»éŒ²ç•ªå·: {detail_info.get('registration_number', 'N/A')}")
                        print(f"     ğŸ¢ æœºæ„åç§°: {detail_info.get('organization_name', org['name'])}")
                        print(f"     ğŸ“ åœ°å€: {detail_info.get('address', 'N/A')}")
                        print(f"     ğŸ¯ æ”¯æ´ç±»å‹: {detail_info.get('support_type', 'N/A')}")
                        print(f"     ğŸŒ ç½‘ç«™: {detail_info.get('website', 'N/A')}")
                        
                        # æ£€æŸ¥æœºæ„æ˜¯å¦å·²å­˜åœ¨
                        exists = scraper.check_organization_exists({
                            'organization_name': detail_info.get('organization_name', org['name']),
                            'registration_number': detail_info.get('registration_number', '')
                        })
                        
                        if exists:
                            print(f"     ğŸ”„ çŠ¶æ€: å·²å­˜åœ¨ï¼Œå°†è·³è¿‡")
                            skipped_count += 1
                        else:
                            print(f"     ğŸ’¾ çŠ¶æ€: æ–°æœºæ„ï¼Œå°†ä¿å­˜")
                            saved_count += 1
                        print()
                    else:
                        print(f"     âŒ æå–å¤±è´¥")
                        
                except Exception as e:
                    print(f"     âŒ å¤„ç†æœºæ„å¤±è´¥: {e}")
                    continue
            
            print(f"âœ… ç¬¬ {page_number} é¡µå¤„ç†å®Œæˆ")
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µ
            next_url = scraper.get_next_page_url(soup)
            if next_url and page_number < max_pages:
                current_url = next_url
                page_number += 1
                print(f"ğŸ”„ å‡†å¤‡å¤„ç†ä¸‹ä¸€é¡µ (ç¬¬ {page_number} é¡µ)\n")
            else:
                if page_number >= max_pages:
                    print(f"â„¹ï¸ å·²è¾¾åˆ°æ¼”ç¤ºé¡µæ•°é™åˆ¶ ({max_pages} é¡µ)")
                else:
                    print("â„¹ï¸ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥")
                break
            
        except Exception as e:
            print(f"âŒ å¤„ç†ç¬¬ {page_number} é¡µå¤±è´¥: {e}")
            break
    
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   ğŸ’¾ æ–°æœºæ„: {saved_count}")
    print(f"   ğŸ”„ å·²å­˜åœ¨: {skipped_count}")
    print(f"   ğŸ“ˆ æ€»å¤„ç†: {saved_count + skipped_count}")

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºWARNINGä»¥å‡å°‘è¾“å‡º
    logging.getLogger().setLevel(logging.WARNING)
    
    run_demo_scrape(max_pages=2, max_orgs_per_page=2)