import logging
import os
import random
import re
import time
from typing import List, Optional, Tuple
from urllib.parse import quote, urljoin, urlparse

import pymysql
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from colorama import Fore, Back, Style, init

# åˆå§‹åŒ–coloramaï¼Œæ”¯æŒWindowsç³»ç»Ÿ
init(autoreset=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# å½©è‰²æ—¥å¿—å·¥å…·ç±»
class ColorLogger:
    @staticmethod
    def success(message: str):
        """æˆåŠŸæ—¥å¿— - ç»¿è‰² + âœ…"""
        print(f"{Fore.GREEN}âœ… {message}{Style.RESET_ALL}")
        logger.info(message)
    
    @staticmethod
    def error(message: str):
        """é”™è¯¯æ—¥å¿— - çº¢è‰² + âŒ"""
        print(f"{Fore.RED}âŒ {message}{Style.RESET_ALL}")
        logger.error(message)
    
    @staticmethod
    def warning(message: str):
        """è­¦å‘Šæ—¥å¿— - é»„è‰² + âš ï¸"""
        print(f"{Fore.YELLOW}âš ï¸  {message}{Style.RESET_ALL}")
        logger.warning(message)
    
    @staticmethod
    def info(message: str):
        """ä¿¡æ¯æ—¥å¿— - è“è‰² + â„¹ï¸"""
        print(f"{Fore.CYAN}â„¹ï¸  {message}{Style.RESET_ALL}")
        logger.info(message)
    
    @staticmethod
    def processing(message: str):
        """å¤„ç†ä¸­æ—¥å¿— - ç´«è‰² + ğŸ”„"""
        print(f"{Fore.MAGENTA}ğŸ”„ {message}{Style.RESET_ALL}")
        logger.info(message)
    
    @staticmethod
    def found(message: str):
        """å‘ç°æ—¥å¿— - é»„è‰² + ğŸ”"""
        print(f"{Fore.YELLOW}ğŸ” {message}{Style.RESET_ALL}")
        logger.info(message)

# åˆ›å»ºå…¨å±€å½©è‰²æ—¥å¿—å®ä¾‹
color_log = ColorLogger()

# åŠ è½½ç¯å¢ƒå˜é‡ï¼Œè®¾ç½®override=Falseï¼Œè¿™æ ·ç³»ç»Ÿç¯å¢ƒå˜é‡ä¼šä¼˜å…ˆäº.envæ–‡ä»¶ä¸­çš„å˜é‡
load_dotenv(override=False)

# åˆ¤æ–­å½“å‰ç¯å¢ƒ
ENVIRONMENT = os.getenv('ENVIRONMENT', 'development')
logger.info(f"å½“å‰è¿è¡Œç¯å¢ƒ: {ENVIRONMENT}")

# å¦‚æœæ˜¯ç”Ÿäº§ç¯å¢ƒï¼Œè¾“å‡ºä¸€ä¸ªæç¤º
if ENVIRONMENT == 'production':
    logger.info("æ­£åœ¨ä½¿ç”¨ç”Ÿäº§ç¯å¢ƒé…ç½®")
else:
    logger.info("æ­£åœ¨ä½¿ç”¨å¼€å‘ç¯å¢ƒé…ç½®")


class SupportOrganizationScraper:
    def __init__(self):
        # æ•°æ®åº“è¿æ¥é…ç½® - ä»ç¯å¢ƒå˜é‡è·å–
        # éªŒè¯å¿…éœ€çš„ç¯å¢ƒå˜é‡
        db_password = os.getenv('DB_APP_PASSWORD')
        if not db_password:
            color_log.error("DB_APP_PASSWORD environment variable is required")
            raise ValueError("DB_APP_PASSWORD environment variable is required")
            
        self.db_config = {
            "host": os.getenv('DB_HOST', 'localhost'),
            "user": os.getenv('DB_APP_USER', 'edm_app_user'),
            "password": db_password,
            "database": os.getenv('DB_NAME', 'edm'),
            "charset": "utf8mb4",
        }

        # ç½‘ç«™åŸºç¡€URL
        self.base_url = "https://www.gai-rou.com"
        self.search_url = "https://www.gai-rou.com/?s={}"

        # è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ja-JP,ja;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

        # åˆ›å»ºä¼šè¯
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        try:
            connection = pymysql.connect(**self.db_config)
            color_log.success(f"æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“ (ç¯å¢ƒ: {ENVIRONMENT})")
            return connection
        except pymysql.Error as err:
            color_log.error(f"æ•°æ®åº“è¿æ¥é”™è¯¯: {err}")
            return None

    def fetch_organizations(self) -> List[Tuple]:
        """ä»æ•°æ®åº“è·å–å…¨æ—¥æœ¬çš„æœºæ„ä¿¡æ¯"""
        connection = self.get_db_connection()
        if not connection:
            return []

        try:
            cursor = connection.cursor()
            query = """
            SELECT id, registration_number, organization_name, email, prefecture
            FROM support_organization_registry 
            WHERE email IS NULL OR email = ''
            ORDER BY id
            """
            cursor.execute(query)
            results = cursor.fetchall()
            color_log.info(f"ä»æ•°æ®åº“è·å–åˆ° {len(results)} æ¡éœ€è¦æŠ“å–é‚®ä»¶çš„è®°å½•")
            return results
        except pymysql.Error as err:
            color_log.error(f"æŸ¥è¯¢æ•°æ®åº“é”™è¯¯: {err}")
            return []
        finally:
            if connection:
                cursor.close()
                connection.close()

    def search_organization(self, registration_number: str) -> Optional[str]:
        """åœ¨ç½‘ç«™ä¸Šæœç´¢æœºæ„å¹¶è·å–è¯¦æƒ…é¡µé¢URL"""
        try:
            # æ„å»ºæœç´¢URL
            search_url = self.search_url.format(quote(registration_number))
            color_log.processing(f"æœç´¢URL: {search_url}")

            # è¯·æ±‚æœç´¢é¡µé¢
            response = self.session.get(search_url, timeout=10)
            response.raise_for_status()

            # è§£æHTML
            soup = BeautifulSoup(response.content, "html.parser")

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾è¯¦æƒ…é¡µé¢é“¾æ¥
            # å¯»æ‰¾ç±»ä¼¼ https://www.gai-rou.com/shien/æ•°å­—/ çš„é“¾æ¥
            detail_pattern = re.compile(r"https://www\.gai-rou\.com/shien/(\d+)/?")

            # åœ¨æ‰€æœ‰é“¾æ¥ä¸­æœç´¢
            for link in soup.find_all("a", href=True):
                href = link["href"]
                if detail_pattern.match(href):
                    color_log.found(f"æ‰¾åˆ°è¯¦æƒ…é¡µé¢é“¾æ¥: {href}")
                    return href

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œæ•´URLï¼Œå°è¯•æŸ¥æ‰¾ç›¸å¯¹è·¯å¾„
            relative_pattern = re.compile(r"/shien/(\d+)/?")
            for link in soup.find_all("a", href=True):
                href = link["href"]
                match = relative_pattern.match(href)
                if match:
                    full_url = urljoin(self.base_url, href)
                    color_log.found(f"æ‰¾åˆ°è¯¦æƒ…é¡µé¢é“¾æ¥ (ç›¸å¯¹è·¯å¾„): {full_url}")
                    return full_url

            color_log.warning(f"æœªæ‰¾åˆ° {registration_number} çš„è¯¦æƒ…é¡µé¢é“¾æ¥")
            return None

        except requests.RequestException as e:
            color_log.error(f"è¯·æ±‚æœç´¢é¡µé¢å¤±è´¥ {registration_number}: {e}")
            return None
        except Exception as e:
            color_log.error(f"æœç´¢è¿‡ç¨‹å‡ºé”™ {registration_number}: {e}")
            return None

    def extract_email_and_website(self, detail_url: str) -> Tuple[Optional[str], Optional[str]]:
        """ä»è¯¦æƒ…é¡µé¢æå–emailåœ°å€å’Œç½‘ç«™URL"""
        try:
            color_log.processing(f"è·å–è¯¦æƒ…é¡µé¢: {detail_url}")
            response = self.session.get(detail_url, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")
            email = None
            website_url = None

            # 1. é¦–å…ˆå°è¯•æå–emailåœ°å€
            email = self.extract_email_from_page(soup)
            
            # 2. æå–ç½‘ç«™URL
            website_url = self.extract_website_from_page(soup)
            
            return email, website_url

        except requests.RequestException as e:
            color_log.error(f"è¯·æ±‚è¯¦æƒ…é¡µé¢å¤±è´¥ {detail_url}: {e}")
            return None, None
        except Exception as e:
            color_log.error(f"æå–ä¿¡æ¯è¿‡ç¨‹å‡ºé”™ {detail_url}: {e}")
            return None, None
    
    def extract_email_from_page(self, soup: BeautifulSoup) -> Optional[str]:
        """ä»é¡µé¢ä¸­æå–emailåœ°å€çš„ä¼˜åŒ–ç®—æ³•"""
        # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«"ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹"ã€"E-mail"ã€"Email"ç­‰å…³é”®è¯çš„å…ƒç´ 
        email_keywords = ["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", "E-mail", "Email", "email", "é€£çµ¡å…ˆ"]
        
        for keyword in email_keywords:
            email_elements = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
            for element in email_elements:
                parent = element.parent
                if parent:
                    # åœ¨çˆ¶å…ƒç´ åŠå…¶å…„å¼Ÿå…ƒç´ ä¸­æŸ¥æ‰¾email
                    for sibling in parent.find_next_siblings(limit=3):
                        email = self.find_email_in_element(sibling)
                        if email:
                            color_log.found(f"é€šè¿‡å…³é”®è¯'{keyword}'æ‰¾åˆ°email: {email}")
                            return email
                    
                    # åœ¨çˆ¶å…ƒç´ çš„ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ ä¸­æŸ¥æ‰¾
                    next_sibling = parent.find_next_sibling()
                    if next_sibling:
                        email = self.find_email_in_element(next_sibling)
                        if email:
                            color_log.found(f"é€šè¿‡å…³é”®è¯'{keyword}'åœ¨å…„å¼Ÿå…ƒç´ ä¸­æ‰¾åˆ°email: {email}")
                            return email
        
        # æ–¹æ³•2: æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥ä¸­çš„mailtoé“¾æ¥
        mailto_links = soup.find_all('a', href=re.compile(r'^mailto:', re.IGNORECASE))
        for link in mailto_links:
            href = link.get('href', '')
            if href.startswith('mailto:'):
                email = href.replace('mailto:', '').split('?')[0]  # ç§»é™¤æŸ¥è¯¢å‚æ•°
                if self.validate_email(email):
                    color_log.found(f"ä»mailtoé“¾æ¥æ‰¾åˆ°email: {email}")
                    return email
        
        # æ–¹æ³•3: åœ¨æ•´ä¸ªé¡µé¢æ–‡æœ¬ä¸­æœç´¢emailæ¨¡å¼ï¼ˆä½†æ’é™¤å¸¸è§çš„æ— ç”¨emailï¼‰
        page_text = soup.get_text()
        email = self.find_email_in_text(page_text)
        if email and not self.is_generic_email(email):
            color_log.found(f"åœ¨é¡µé¢æ–‡æœ¬ä¸­æ‰¾åˆ°email: {email}")
            return email
        
        return None
    
    def extract_website_from_page(self, soup: BeautifulSoup) -> Optional[str]:
        """ä»é¡µé¢ä¸­æå–ç½‘ç«™URL"""
        # æŸ¥æ‰¾åŒ…å«"ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"ã€"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ"ã€"URL"ç­‰å…³é”®è¯çš„å…ƒç´ 
        website_keywords = ["ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸", "ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ", "URL", "Website", "ã‚µã‚¤ãƒˆ"]
        
        for keyword in website_keywords:
            website_elements = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
            for element in website_elements:
                parent = element.parent
                if parent:
                    # åœ¨çˆ¶å…ƒç´ åŠå…¶å…„å¼Ÿå…ƒç´ ä¸­æŸ¥æ‰¾URL
                    for sibling in parent.find_next_siblings(limit=3):
                        url = self.find_url_in_element(sibling)
                        if url:
                            color_log.found(f"é€šè¿‡å…³é”®è¯'{keyword}'æ‰¾åˆ°ç½‘ç«™: {url}")
                            return url
        
        # æŸ¥æ‰¾æ‰€æœ‰å¤–éƒ¨é“¾æ¥
        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href', '')
            if self.is_external_website(href):
                color_log.found(f"æ‰¾åˆ°å¤–éƒ¨ç½‘ç«™é“¾æ¥: {href}")
                return href
        
        return None
    
    def find_url_in_element(self, element) -> Optional[str]:
        """åœ¨HTMLå…ƒç´ ä¸­æŸ¥æ‰¾URL"""
        if element:
            # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯é“¾æ¥å…ƒç´ 
            if element.name == 'a' and element.get('href'):
                href = element.get('href')
                if self.is_external_website(href):
                    return href
            
            # åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾URL
            text = element.get_text()
            url = self.find_url_in_text(text)
            if url:
                return url
        return None
    
    def find_url_in_text(self, text: str) -> Optional[str]:
        """åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾URL"""
        url_pattern = re.compile(
            r'https?://[\w\-\.]+(:[0-9]+)?(/[\w\-\._~:/?#[\]@!$&\'()*+,;=]*)?',
            re.IGNORECASE
        )
        matches = url_pattern.findall(text)
        
        for match in matches:
            if isinstance(match, tuple):
                url = match[0] + (match[1] if match[1] else '') + (match[2] if match[2] else '')
            else:
                url = match
            
            if self.is_external_website(url):
                return url
        return None
    
    def is_external_website(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å¤–éƒ¨ç½‘ç«™URL"""
        if not url or not url.startswith(('http://', 'https://')):
            return False
        
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # æ’é™¤gai-rou.comåŸŸå
            if 'gai-rou.com' in domain:
                return False
            
            # æ’é™¤å¸¸è§çš„ç¤¾äº¤åª’ä½“å’Œæœç´¢å¼•æ“
            excluded_domains = [
                'facebook.com', 'twitter.com', 'instagram.com', 'youtube.com',
                'google.com', 'yahoo.co.jp', 'bing.com'
            ]
            
            for excluded in excluded_domains:
                if excluded in domain:
                    return False
            
            return True
        except:
            return False
    
    def is_generic_email(self, email: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯é€šç”¨é‚®ç®±åœ°å€ï¼ˆéœ€è¦æ’é™¤çš„ï¼‰"""
        generic_patterns = [
            r'.*@(example|test|sample)\.com',
            r'.*@gmail\.com',  # å¯é€‰æ‹©æ˜¯å¦æ’é™¤gmail
            r'.*@yahoo\.(com|co\.jp)',
            r'.*@hotmail\.com',
            r'noreply@.*',
            r'no-reply@.*'
        ]
        
        for pattern in generic_patterns:
            if re.match(pattern, email.lower()):
                return True
        return False
    
    def scrape_email_from_website(self, website_url: str, max_pages: int = 3) -> Optional[str]:
        """ä»å…¬å¸å®˜ç½‘æŠ“å–é‚®ä»¶åœ°å€"""
        if not website_url:
            return None
        
        try:
            color_log.processing(f"å¼€å§‹ä»å®˜ç½‘æŠ“å–é‚®ä»¶: {website_url}")
            
            # è¦æ£€æŸ¥çš„é¡µé¢åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            pages_to_check = []
            
            # 1. ä¸»é¡µ
            pages_to_check.append(website_url.rstrip('/'))
            
            # 2. å¸¸è§çš„è”ç³»é¡µé¢
            contact_paths = [
                '/contact', '/contact.html', '/contact.php', '/contact.htm',
                '/ãŠå•ã„åˆã‚ã›', '/ã‚³ãƒ³ã‚¿ã‚¯ãƒˆ',
                '/about', '/about.html', '/about.php', '/about.htm',
                '/company', '/company.html', '/company.php',
                '/access', '/access.html'
            ]
            
            base_url = website_url.rstrip('/')
            for path in contact_paths[:max_pages-1]:  # é™¤äº†ä¸»é¡µå¤–çš„å…¶ä»–é¡µé¢
                pages_to_check.append(base_url + path)
            
            # æ£€æŸ¥æ¯ä¸ªé¡µé¢
            for page_url in pages_to_check:
                email = self.extract_email_from_website_page(page_url)
                if email:
                    color_log.success(f"åœ¨å®˜ç½‘é¡µé¢æ‰¾åˆ°é‚®ä»¶: {email} (é¡µé¢: {page_url})")
                    return email
                
                # æ¯ä¸ªé¡µé¢ä¹‹é—´æ·»åŠ å°çš„å»¶è¿Ÿ
                time.sleep(random.uniform(0.2, 0.5))
            
            color_log.warning(f"æœªåœ¨å®˜ç½‘æ‰¾åˆ°é‚®ä»¶åœ°å€: {website_url}")
            return None
            
        except Exception as e:
            color_log.error(f"ä»å®˜ç½‘æŠ“å–é‚®ä»¶æ—¶å‡ºé”™ {website_url}: {e}")
            return None
    
    def extract_email_from_website_page(self, page_url: str) -> Optional[str]:
        """ä»å•ä¸ªç½‘ç«™é¡µé¢æå–é‚®ä»¶åœ°å€"""
        try:
            logger.debug(f"æ£€æŸ¥é¡µé¢: {page_url}")
            response = self.session.get(page_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, "html.parser")
            
            # ä½¿ç”¨ç›¸åŒçš„é‚®ä»¶æå–ç®—æ³•
            email = self.extract_email_from_page(soup)
            
            if email and not self.is_generic_email(email):
                return email
            
            return None
            
        except requests.RequestException as e:
            logger.debug(f"è¯·æ±‚é¡µé¢å¤±è´¥ {page_url}: {e}")
            return None
        except Exception as e:
            logger.debug(f"å¤„ç†é¡µé¢æ—¶å‡ºé”™ {page_url}: {e}")
            return None
    
    def retry_request(self, func, *args, max_retries: int = 3, **kwargs):
        """é‡è¯•æœºåˆ¶è£…é¥°å™¨"""
        for attempt in range(max_retries):
            try:
                return func(*args, **kwargs)
            except requests.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = random.uniform(1, 3) * (attempt + 1)
                    color_log.warning(f"è¯·æ±‚å¤±è´¥ï¼Œ{wait_time:.1f}ç§’åé‡è¯• (ç¬¬{attempt + 1}æ¬¡): {e}")
                    time.sleep(wait_time)
                else:
                    color_log.error(f"é‡è¯•{max_retries}æ¬¡åä»ç„¶å¤±è´¥: {e}")
                    raise e
            except Exception as e:
                color_log.error(f"éç½‘ç»œé”™è¯¯ï¼Œä¸é‡è¯•: {e}")
                raise e
        return None

    def find_email_in_element(self, element) -> Optional[str]:
        """åœ¨HTMLå…ƒç´ ä¸­æŸ¥æ‰¾emailåœ°å€"""
        if element:
            text = element.get_text()
            return self.find_email_in_text(text)
        return None

    def find_email_in_text(self, text: str) -> Optional[str]:
        """åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾emailåœ°å€"""
        # emailæ­£åˆ™è¡¨è¾¾å¼
        email_pattern = re.compile(
            r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"
        )
        matches = email_pattern.findall(text)

        if matches:
            # è¿”å›ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„emailåœ°å€
            email = matches[0]
            if self.validate_email(email):
                return email
        return None

    def validate_email(self, email: str) -> bool:
        """éªŒè¯emailåœ°å€çš„åˆæ³•æ€§"""
        pattern = re.compile(r"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}$")
        return bool(pattern.match(email))

    def update_email_in_db(self, org_id: int, email: str) -> bool:
        """æ›´æ–°æ•°æ®åº“ä¸­çš„emailåœ°å€"""
        connection = self.get_db_connection()
        if not connection:
            return False

        try:
            cursor = connection.cursor()
            query = "UPDATE support_organization_registry SET email = %s WHERE id = %s"
            cursor.execute(query, (email, org_id))
            connection.commit()

            if cursor.rowcount > 0:
                color_log.success(f"æˆåŠŸæ›´æ–°æœºæ„ID {org_id} çš„email: {email}")
                return True
            else:
                color_log.warning(f"æ›´æ–°æœºæ„ID {org_id} çš„emailå¤±è´¥ï¼šæœªæ‰¾åˆ°è®°å½•")
                return False

        except pymysql.Error as err:
            color_log.error(f"æ›´æ–°æ•°æ®åº“é”™è¯¯: {err}")
            connection.rollback()
            return False
        finally:
            if connection:
                cursor.close()
                connection.close()

    def process_organizations(self):
        """å¤„ç†æ‰€æœ‰æœºæ„çš„ä¸»æµç¨‹"""
        # è·å–æœºæ„åˆ—è¡¨
        organizations = self.fetch_organizations()
        if not organizations:
            color_log.error("æœªè·å–åˆ°ä»»ä½•æœºæ„æ•°æ®")
            return

        success_count = 0
        failed_count = 0

        for (
            org_id,
            registration_number,
            organization_name,
            current_email,
            prefecture,
        ) in organizations:
            color_log.processing(
                f"å¤„ç†æœºæ„: ID={org_id}, ç™»éŒ²ç•ªå·={registration_number}, åç§°={organization_name}, éƒ½é“åºœçœŒ={prefecture}"
            )

            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æœ‰æ•ˆçš„emailåœ°å€
            if current_email and current_email.strip():
                color_log.info(
                    f"æœºæ„ {organization_name} å·²æœ‰emailåœ°å€: {current_email}ï¼Œè·³è¿‡"
                )
                continue

            try:
                # æœç´¢æœºæ„è¯¦æƒ…é¡µé¢
                detail_url = self.retry_request(self.search_organization, registration_number)
                if not detail_url:
                    color_log.warning(f"æœªæ‰¾åˆ°æœºæ„ {organization_name} çš„è¯¦æƒ…é¡µé¢")
                    failed_count += 1
                    continue

                # ä» gai-rou.com æå–emailå’Œç½‘ç«™
                email, website_url = self.retry_request(self.extract_email_and_website, detail_url)
                
                # å¦‚æœåœ¨ gai-rou.com æ²¡æ‰¾åˆ°emailï¼Œå°è¯•ä»å®˜ç½‘æŠ“å–
                if not email and website_url:
                    color_log.info(f"åœ¨gai-rou.comæœªæ‰¾åˆ°emailï¼Œå°è¯•ä»å®˜ç½‘æŠ“å–: {website_url}")
                    email = self.scrape_email_from_website(website_url)
                
                if not email:
                    color_log.error(f"æœªæ‰¾åˆ°æœºæ„ {organization_name} çš„emailåœ°å€")
                    failed_count += 1
                    continue

                # æ›´æ–°æ•°æ®åº“
                if self.update_email_in_db(org_id, email):
                    success_count += 1
                    source = "gai-rou.com" if not website_url else "å®˜ç½‘"
                    color_log.success(f"âœ¨ æˆåŠŸå¤„ç†æœºæ„ {organization_name}: {email} (æ¥æº: {source})")
                else:
                    failed_count += 1
                    color_log.error(f"æ›´æ–°æœºæ„ {organization_name} çš„emailå¤±è´¥")

                # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                delay = random.uniform(0.5, 2.0)
                logger.debug(f"ç­‰å¾… {delay:.2f} ç§’")
                time.sleep(delay)

            except Exception as e:
                color_log.error(f"å¤„ç†æœºæ„ {organization_name} æ—¶å‡ºé”™: {e}")
                failed_count += 1

        # æœ€ç»ˆç»“æœç»Ÿè®¡
        total = success_count + failed_count
        success_rate = (success_count / total * 100) if total > 0 else 0
        
        print(f"\n{Fore.WHITE}{Back.GREEN} å¤„ç†å®Œæˆ {Style.RESET_ALL}")
        print(f"{Fore.GREEN}âœ… æˆåŠŸ: {success_count}{Style.RESET_ALL}")
        print(f"{Fore.RED}âŒ å¤±è´¥: {failed_count}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%{Style.RESET_ALL}")
        
        color_log.success(f"å¤„ç†å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}, æˆåŠŸç‡: {success_rate:.1f}%")


def main():
    """ä¸»å‡½æ•°"""
    print(f"\n{Fore.CYAN}{'=' * 60}{Style.RESET_ALL}")
    print(f"{Fore.WHITE}{Back.BLUE}           ğŸš€ EDMé‚®ä»¶åœ°å€æŠ“å–ç¨‹åº (ç¯å¢ƒ: {ENVIRONMENT})           {Style.RESET_ALL}")
    print(f"{Fore.CYAN}{'=' * 60}{Style.RESET_ALL}\n")
    
    scraper = SupportOrganizationScraper()

    # æ˜¾ç¤ºå½“å‰æ•°æ®åº“é…ç½®
    print(f"{Fore.YELLOW}ğŸ“‹ å½“å‰æ•°æ®åº“é…ç½®:{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ä¸»æœº: {Fore.GREEN}{os.getenv('DB_HOST', 'localhost')}{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ æ•°æ®åº“: {Fore.GREEN}{os.getenv('DB_NAME', 'edm')}{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ç”¨æˆ·: {Fore.GREEN}{os.getenv('DB_APP_USER', 'edm_app_user')}{Style.RESET_ALL}")
    
    # ç¡®è®¤æ˜¯å¦ç»§ç»­
    print(f"\n{Fore.YELLOW}âš¡ åŠŸèƒ½ç‰¹æ€§:{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ğŸŒ æ”¯æŒå…¨æ—¥æœ¬åœ°åŒºæŠ“å–{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ğŸ”„ æ™ºèƒ½é‡è¯•æœºåˆ¶{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ğŸŒ å®˜ç½‘å¤‡ç”¨æŠ“å–{Style.RESET_ALL}")
    print(f"{Fore.WHITE}  â€¢ ğŸ¯ éšæœºè¯·æ±‚é—´éš”{Style.RESET_ALL}")
    
    response = input(f"\n{Fore.CYAN}ğŸš€ æ˜¯å¦å¼€å§‹å¤„ç†? (y/N): {Style.RESET_ALL}").strip().lower()
    if response != "y":
        print(f"{Fore.YELLOW}â¹ï¸  æ“ä½œå–æ¶ˆ{Style.RESET_ALL}")
        return

    # å¼€å§‹å¤„ç†
    print(f"\n{Fore.GREEN}ğŸ¯ å¼€å§‹æŠ“å–é‚®ä»¶åœ°å€...{Style.RESET_ALL}\n")
    scraper.process_organizations()


if __name__ == "__main__":
    main()
