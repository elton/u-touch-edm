#!/usr/bin/env python3
"""
æµ‹è¯•åŠ¨æ€åˆ†é¡µåŠŸèƒ½
éªŒè¯çˆ¬è™«æ˜¯å¦èƒ½æ­£ç¡®è·Ÿéš"ä¸‹ä¸€é¡µ"é“¾æ¥

ä½œè€…: Claude
æ—¥æœŸ: 2025-07-31
"""

from gai_rou_scraper import GaiRouScraper, color_log, Fore, Back, Style

def test_next_page_detection():
    """æµ‹è¯•ä¸‹ä¸€é¡µé“¾æ¥æ£€æµ‹åŠŸèƒ½"""
    
    print(f"\n{Fore.CYAN}{'=' * 60}{Style.RESET_ALL}")
    print(f"{Fore.WHITE}{Back.BLUE}      ğŸ” æµ‹è¯•åŠ¨æ€åˆ†é¡µåŠŸèƒ½      {Style.RESET_ALL}")
    print(f"{Fore.CYAN}{'=' * 60}{Style.RESET_ALL}\n")
    
    scraper = GaiRouScraper()
    
    # æµ‹è¯•å‰å‡ é¡µçš„é“¾æ¥æ£€æµ‹
    test_urls = [
        "https://www.gai-rou.com/shien_list/",
        "https://www.gai-rou.com/shien_list/page/2/",
        "https://www.gai-rou.com/shien_list/page/3/"
    ]
    
    for i, url in enumerate(test_urls, 1):
        color_log.processing(f"æµ‹è¯•ç¬¬ {i} ä¸ªURL: {url}")
        
        soup = scraper.get_page_content(url)
        if soup:
            # æ£€æŸ¥æœºæ„æ•°é‡
            organizations = scraper.extract_organization_list(soup)
            color_log.found(f"å‘ç° {len(organizations)} ä¸ªæœºæ„")
            
            # æ£€æŸ¥ä¸‹ä¸€é¡µé“¾æ¥
            next_url = scraper.get_next_page_url(soup)
            if next_url:
                color_log.success(f"âœ… æˆåŠŸæ‰¾åˆ°ä¸‹ä¸€é¡µ: {next_url}")
            else:
                color_log.warning("âš ï¸ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥")
        else:
            color_log.error("âŒ è·å–é¡µé¢å¤±è´¥")
        
        print()  # ç©ºè¡Œåˆ†éš”

def test_limited_pagination():
    """æµ‹è¯•æœ‰é™çš„åˆ†é¡µçˆ¬å–ï¼ˆåªçˆ¬å–å‰å‡ é¡µï¼‰"""
    
    print(f"{Fore.YELLOW}ğŸ§ª æµ‹è¯•æœ‰é™åˆ†é¡µçˆ¬å– (å‰3é¡µ){Style.RESET_ALL}\n")
    
    scraper = GaiRouScraper()
    
    # ä¸´æ—¶ä¿®æ”¹çˆ¬å–é€»è¾‘ï¼Œåªå¤„ç†å‰3é¡µ
    current_url = scraper.list_url
    page_number = 1
    max_test_pages = 3
    
    total_found = 0
    
    while current_url and page_number <= max_test_pages:
        color_log.processing(f"æµ‹è¯•ç¬¬ {page_number}/{max_test_pages} é¡µ: {current_url}")
        
        soup = scraper.get_page_content(current_url)
        if not soup:
            color_log.error(f"è·å–ç¬¬ {page_number} é¡µå¤±è´¥")
            break
        
        # æå–æœºæ„åˆ—è¡¨
        organizations = scraper.extract_organization_list(soup)
        color_log.found(f"ç¬¬ {page_number} é¡µå‘ç° {len(organizations)} ä¸ªæœºæ„")
        total_found += len(organizations)
        
        # æ˜¾ç¤ºå‰3ä¸ªæœºæ„åç§°
        if organizations:
            print(f"{Fore.WHITE}  å‰3ä¸ªæœºæ„:{Style.RESET_ALL}")
            for j, org in enumerate(organizations[:3], 1):
                print(f"{Fore.CYAN}    {j}. {org['name'][:80]}...{Style.RESET_ALL}")
        
        # æŸ¥æ‰¾ä¸‹ä¸€é¡µ
        next_url = scraper.get_next_page_url(soup)
        if next_url and page_number < max_test_pages:
            current_url = next_url
            page_number += 1
        else:
            if page_number >= max_test_pages:
                color_log.info(f"å·²è¾¾åˆ°æµ‹è¯•é¡µæ•°é™åˆ¶ ({max_test_pages} é¡µ)")
            else:
                color_log.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥")
            break
        
        print()  # ç©ºè¡Œåˆ†éš”
    
    print(f"\n{Fore.WHITE}{Back.GREEN} ğŸ“Š æµ‹è¯•ç»“æœ {Style.RESET_ALL}")
    print(f"{Fore.GREEN}æµ‹è¯•é¡µæ•°: {page_number} é¡µ{Style.RESET_ALL}")
    print(f"{Fore.CYAN}å‘ç°æœºæ„æ€»æ•°: {total_found} ä¸ª{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}å¹³å‡æ¯é¡µ: {total_found/page_number:.1f} ä¸ªæœºæ„{Style.RESET_ALL}")

if __name__ == "__main__":
    try:
        # æµ‹è¯•ä¸‹ä¸€é¡µæ£€æµ‹
        test_next_page_detection()
        
        # æµ‹è¯•æœ‰é™åˆ†é¡µ
        test_limited_pagination()
        
        print(f"\n{Fore.WHITE}{Back.GREEN} ğŸ‰ åˆ†é¡µæµ‹è¯•å®Œæˆ! {Style.RESET_ALL}")
        
    except Exception as e:
        color_log.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        exit(1)